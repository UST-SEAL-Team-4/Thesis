{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from project.preprocessing import z_score_normalization, min_max_normalization, NiftiToTensorTransform\n",
    "# from project.preprocessing import z_score_normalization, min_max_normalization\n",
    "from project.training import split_train_val_datasets\n",
    "from project.utils import collate_fn, plot_all_slices, plot_all_slices_from_array\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from project.model import VisionTransformer\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from project.model.rpn_to_gcvit import RPN_to_GCVIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_model = 256\n",
    "N_classes = 2\n",
    "Img_size = (16,16)\n",
    "Patch_size = (16, 16)\n",
    "N_channels = 1\n",
    "N_heads = 8\n",
    "N_layers = 3\n",
    "batch_size = 1\n",
    "epochs = 3\n",
    "Alpha = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = ds.load_cmb_masks()\n",
    "cases = ds.load_skullstripped_mri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = NiftiToTensorTransform(target_shape=(512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VALDODataset(\n",
    "    cases=cases, \n",
    "    masks=masks, \n",
    "    transform=transform,\n",
    "    normalization=z_score_normalization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cmb = [1 if count > 0 else 0 for count in dataset.cmb_counts]\n",
    "\n",
    "df_dataset = pd.DataFrame({\n",
    "    'MRI Scans': dataset.cases,\n",
    "    'Segmented Masks': dataset.masks,\n",
    "    'CMB Count': dataset.cmb_counts,\n",
    "    'Has CMB': has_cmb\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_train_val_datasets(\n",
    "    df=df_dataset, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    shuffle=False, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = VisionTransformer(\n",
    "    D_model=D_model,\n",
    "    N_classes=N_classes,\n",
    "    Img_size=Img_size,\n",
    "    Patch_size=Patch_size,\n",
    "    N_channels=N_channels,\n",
    "    N_heads=N_heads,\n",
    "    N_layers=N_layers,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer.to(device)\n",
    "connector = RPN_to_GCVIT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(transformer.parameters(), lr=Alpha)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 loss: 0.000:   0%|          | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "-----------------------\n",
      "Learning case 0 slice 0\n",
      "Shape before patch: torch.Size([1, 1, 49, 16, 16])\n",
      "before torch.Size([1, 1, 49, 16, 16])\n",
      "after view torch.Size([1, 49, 256])\n",
      "after project torch.Size([1, 49, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 loss: 0.000:   0%|          | 0/57 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after patch: torch.Size([1, 49, 256])\n",
      "Shape after positional: torch.Size([1, 50, 256])\n",
      "Shape after transformer: torch.Size([1, 50, 256])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 256]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning case \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m slice \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# outputs = F.interpolate(\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     outputs,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#     size=img_size,\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     mode='bilinear',\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#     align_corners=False\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cropped_labels\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m N_classes:\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\Documents\\GitHub\\Thesis_Folder\\Thesis\\project\\model\\gcvit.py:307\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, images, mask, current_slice)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mprint\u001b[39m(x[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, current_slice]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    306\u001b[0m temp \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, current_slice]\n\u001b[1;32m--> 307\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mtemp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_height\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(temp)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape after with current slice:\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 1, 256]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0.0\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader, 0),\n",
    "        total=len(train_loader),\n",
    "        desc=f'Epoch {epoch + 1}/{epochs} loss: {training_loss / len(train_loader):.3f}'\n",
    "    )\n",
    "\n",
    "    epoch_loss_history = []\n",
    "    \n",
    "    for i, data in progress_bar:\n",
    "\n",
    "        cropped_images = connector.get_cropped_locations(img=data[0], x_min=160, y_min=324, x_max=176, y_max=340).float().to(device)\n",
    "        cropped_labels = connector.get_cropped_locations(img=data[1], x_min=160, y_min=324, x_max=176, y_max=340).float().to(device)\n",
    "\n",
    "        num_slices = cropped_images.size(2)\n",
    "        print(num_slices)\n",
    "        for j in range(num_slices):\n",
    "\n",
    "            print('-----------------------')\n",
    "            print(f'Learning case {i} slice {j}')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = transformer(cropped_images, cropped_labels, current_slice=j)\n",
    "            # outputs = F.interpolate(\n",
    "            #     outputs,\n",
    "            #     size=img_size,\n",
    "            #     mode='bilinear',\n",
    "            #     align_corners=False\n",
    "            # )\n",
    "            \n",
    "            if cropped_labels.max() >= N_classes:\n",
    "                cropped_labels = torch.clamp(cropped_labels, 0, N_classes-1)\n",
    "\n",
    "            if epoch == 2:\n",
    "                for i in range(outputs.shape[0]):  # Assuming outputs[0] is the batch dimension\n",
    "                    img = outputs[0][i].cpu().detach().numpy()  # Move to CPU and convert to NumPy\n",
    "                    plt.imshow(img.squeeze(), cmap='gray')  # Squeeze to remove single-dimensional entries\n",
    "                    plt.title(f'Slice {i}')\n",
    "                    plt.show()\n",
    "\n",
    "            print(outputs.shape)\n",
    "            batch, channel, num_slices, height, width = cropped_images.shape\n",
    "            cropped_labels = cropped_labels.view(batch_size, height * num_slices, width)\n",
    "            # cropped_labels = cropped_labels.long()\n",
    "            loss = criterion(outputs, cropped_labels.long())\n",
    "            epoch_loss_history.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "        \n",
    "    loss_history.append(epoch_loss_history)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "# Frequency (Hz) and duration (ms)\n",
    "frequency = 1000  # Set frequency to 1000 Hz\n",
    "duration = 500    # Set duration to 500 ms\n",
    "\n",
    "# Play the sound\n",
    "winsound.Beep(frequency, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
