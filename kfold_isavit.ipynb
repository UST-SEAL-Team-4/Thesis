{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from project.preprocessing import NiftiToTensorTransform, get_transform\n",
    "from project.utils import collatev2, compute_statistics\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from project.model import VisionTransformer, ISAVIT\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from project.model.feeder import Feeder\n",
    "import seaborn as sns\n",
    "from project.evaluation import isa_vit_metric, Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime as dtt\n",
    "import os\n",
    "\n",
    "path = 'logs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "os.makedirs('history', exist_ok=True)\n",
    "rn = dtt.now()\n",
    "dte = rn.strftime('%b_%d_%Y_%H%M%S')\n",
    "\n",
    "logger = logging.getLogger('andy')\n",
    "fh = logging.FileHandler(f'logs/{dte}.log')\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "\n",
    "tk.date = rn\n",
    "tk.logfile = f'{dte}.log'\n",
    "\n",
    "dte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tk.device = device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config for fitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 300\n",
    "patch_size = 16\n",
    "\n",
    "config = {\n",
    "    'model': ISAVIT(\n",
    "        d_model=512,\n",
    "        patch_size=patch_size,\n",
    "        dim_ff=1600\n",
    "    ).to(device),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'device': device,\n",
    "    'epochs': 5,\n",
    "    'loss': nn.BCEWithLogitsLoss(),\n",
    "    # 'loss': nn.MSELoss(),\n",
    "    'lr': 0.0001\n",
    "}\n",
    "\n",
    "tk.model = 'ViT'\n",
    "tk.model_hyperparams = config['model'].config\n",
    "tk.optimizer = f\"{config['optimizer']}\"\n",
    "tk.epochs = config['epochs']\n",
    "tk.loss = f\"{config['loss']}\"\n",
    "tk.lr = config['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stage 1 Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.model import Feeder, RPN, GCRPN\n",
    "\n",
    "resize = get_transform(\n",
    "    height=patch_size,\n",
    "    width=patch_size,\n",
    "    p=1.0,\n",
    "    rpn_mode=False\n",
    ")\n",
    "\n",
    "feeder = Feeder(resize)\n",
    "rpn = RPN(\n",
    "    input_dim=512,\n",
    "    output_dim=4,\n",
    "    image_size=image_size,\n",
    "    nh=4\n",
    ")\n",
    "\n",
    "tk.uses_resnet = rpn.config['resnet']\n",
    "\n",
    "stone = GCRPN(\n",
    "    rpn=rpn,\n",
    "    feeder=feeder,\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stow = 'rpn_kfold/Nov_17_2024_092857/RPN_Nov_17_2024_092857_fold_1.pt'\n",
    "tk.stage1_weights = stow\n",
    "stone.rpn.load_state_dict(torch.load(stow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ViT Weights\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = config['model']\n",
    "s = 'vit_weights_241024213949.pt'\n",
    "tk.loaded_weights = s\n",
    "model.load_state_dict(torch.load(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()\n",
    "\n",
    "data = pd.read_csv('targets.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('has_microbleed_slice == 1').reset_index(drop=True)\n",
    "tk.only_cmb_slices = True\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader` Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(data, col):\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    iqr = q3-q1\n",
    "    new = data[(data[col] < (q3 + 1.5*iqr)) & (data[col] > (q1 - 1.5*iqr))]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_loaders(data,\n",
    "                 cohort,\n",
    "                 batch_size,\n",
    "                 test_size=0.2,\n",
    "                 random_state=12,\n",
    "                 target_shape=(300, 300),\n",
    "                 rpn_mode=True,\n",
    "                 logger=None,\n",
    "                 tracker=tk\n",
    "                ):\n",
    "    if cohort == 1:\n",
    "        tracker.cohort1 = True\n",
    "    if cohort == 2:\n",
    "        tracker.cohort2 = True\n",
    "    if cohort == 3:\n",
    "        tracker.cohort3 = True\n",
    "    tracker.batch_size = batch_size\n",
    "    tracker.test_size = test_size\n",
    "    tracker.target_shape = target_shape\n",
    "    data = data[data.cohort == cohort]\n",
    "    # data = iqr(data, 'max_value')\n",
    "    \n",
    "    s = f'Creating loaders for Cohort {cohort}\\n'\n",
    "\n",
    "    data_train, data_test = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    s += f'TRAIN & TEST: {data_train.shape, data_test.shape}\\n'\n",
    "\n",
    "    paths = data_train.stripped.unique().tolist()\n",
    "    s += f'Total Unique MRI Samples in data_train: {len(paths)}\\n'\n",
    "    \n",
    "    global_min, global_max = compute_statistics(paths)\n",
    "    s += f'GLOBAL MIN & MAX {global_min, global_max}\\n'\n",
    "\n",
    "    transform = NiftiToTensorTransform(\n",
    "        target_shape=target_shape,\n",
    "        rpn_mode=rpn_mode,\n",
    "        normalization=(global_min, global_max)\n",
    "    )\n",
    "\n",
    "    train_set = VALDODataset(\n",
    "        cases=data_train.stripped.tolist(),\n",
    "        masks=data_train.masks.tolist(),\n",
    "        target=data_train.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "    val_set = VALDODataset(\n",
    "        cases=data_test.stripped.tolist(),\n",
    "        masks=data_test.masks.tolist(),\n",
    "        target=data_test.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "\n",
    "    if logger != None:\n",
    "        logger.info(s)\n",
    "    else:\n",
    "        print(s)\n",
    "    \n",
    "    return train_loader, val_loader, train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter\n",
    "\n",
    "class ViTFitter(Fitter):\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, stage1):\n",
    "        train_history = []\n",
    "        val_history = []\n",
    "        train_metric_history = []\n",
    "        val_metric_history = []\n",
    "        for epoch in range(self.epochs):\n",
    "            self.log(f'EPOCH {epoch} ==============================')\n",
    "            train_loss, train_metric = self.train_one_epoch(train_loader, stage1)\n",
    "            val_loss, val_metric = self.validation(val_loader, stage1)\n",
    "            train_history.append(train_loss)\n",
    "            val_history.append(val_loss)\n",
    "            train_metric_history.append(train_metric)\n",
    "            val_metric_history.append(val_metric)\n",
    "        return train_history, val_history, train_metric_history, val_metric_history\n",
    "\n",
    "    def train_one_epoch(self, train_loader, stage1):\n",
    "        self.model.train()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        counter = 0\n",
    "        for batch in train_loader:\n",
    "            Y = []\n",
    "            T = []\n",
    "            for slices, masks, target, case in batch:\n",
    "                slices = slices.squeeze(1).float().to(self.device)\n",
    "                masks = masks.float().to(self.device)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                \n",
    "                # self.log(f'{x.requires_grad}, {t.requires_grad}')\n",
    "                # self.log(f'{x.shape}, {t.shape}')\n",
    "\n",
    "                x = x.flatten(2).float().to(self.device)\n",
    "                t = t.flatten(2).float().to(self.device)\n",
    "                # self.log(f'XT SHAPES: {x.shape}, {t.shape}')\n",
    "                \n",
    "                y = self.model(x, target)\n",
    "                \n",
    "                # print('Prediction:', (y.sigmoid() >= 0.5).int().unique())\n",
    "                # print('Truth:', (t[target] >= 0).int().unique())\n",
    "\n",
    "                dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().squeeze().detach().cpu().numpy() >= 0.5), (t[target].squeeze().detach().cpu().numpy() > 0))\n",
    "\n",
    "                evaluation_metric['dice_score'].append(dice_score)\n",
    "                evaluation_metric['precision_score'].append(precision_score)\n",
    "                evaluation_metric['recall_score'].append(recall_score)\n",
    "                evaluation_metric['f1_score'].append(f1_score)\n",
    "                evaluation_metric['fpr'].append(fpr)\n",
    "                # self.log(f'EVAL METS: {dice_score, precision_score, recall_score, f1_score, fpr}')\n",
    "\n",
    "                Y.append(y)\n",
    "                T.append(t[target])\n",
    "            \n",
    "            losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "            counter += 1\n",
    "            self.log(f'Batch:\\t{counter}/{len(train_loader)}')\n",
    "            self.log(f'Batch samples:\\t{len(batch)}')\n",
    "            self.log(f'Current error:\\t{losses}\\n')\n",
    "            \n",
    "            loss_history.append(losses.detach().cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nTraining Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        return loss_history, evaluation_metric\n",
    "    \n",
    "    def validation(self, val_loader, stage1):\n",
    "        self.model.eval()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        with torch.inference_mode():\n",
    "            for batch in val_loader:\n",
    "                Y = []\n",
    "                T = []\n",
    "                for slices, masks, target, case in batch:\n",
    "                    slices = slices.squeeze(1).float().to(self.device)\n",
    "                    masks = masks.float().to(self.device)\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                    x = x.flatten(2).float().to(self.device)\n",
    "                    t = t.flatten(2).float().to(self.device)\n",
    "                    y = self.model(x, target)\n",
    "\n",
    "                    dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().squeeze().detach().cpu().numpy() >= 0.5), (t[target].squeeze().detach().cpu().numpy() > 0))\n",
    "                    evaluation_metric['dice_score'].append(dice_score)\n",
    "                    evaluation_metric['precision_score'].append(precision_score)\n",
    "                    evaluation_metric['recall_score'].append(recall_score)\n",
    "                    evaluation_metric['f1_score'].append(f1_score)\n",
    "                    evaluation_metric['fpr'].append(fpr)\n",
    "                    Y.append(y)\n",
    "                    T.append(t[target])\n",
    "                \n",
    "                losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "                loss_history.append(losses.cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nValidations Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        return loss_history, evaluation_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, t1, v1 = make_loaders(\n",
    "    data=data,\n",
    "    cohort=1,\n",
    "    batch_size=20,\n",
    "    rpn_mode=False\n",
    ")\n",
    "\n",
    "_, _, t3, v3 = make_loaders(\n",
    "    data=data,\n",
    "    cohort=3,\n",
    "    batch_size=20,\n",
    "    rpn_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_set = ConcatDataset([t1, t3])\n",
    "test_set = ConcatDataset([v1, v3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=45)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics = []\n",
    "vit_kfold_dir = f'./vit_kfold/{dte}'\n",
    "os.makedirs(vit_kfold_dir, exist_ok=True)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(train_set)):\n",
    "    print(f'Fold {i + 1}/{5}')\n",
    "    \n",
    "    config['model'] = ISAVIT(\n",
    "        d_model=512,\n",
    "        patch_size=patch_size,\n",
    "        dim_ff=1600\n",
    "    ).to(device)\n",
    "    \n",
    "    fitter = ViTFitter(config, logger=logger)\n",
    "    \n",
    "    train_subset = Subset(train_set, train_index)\n",
    "    val_subset = Subset(train_set, val_index)\n",
    "    \n",
    "    train_subset_dl = DataLoader(\n",
    "        train_subset,\n",
    "        shuffle=True,\n",
    "        batch_size=20,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    \n",
    "    val_subset_dl = DataLoader(\n",
    "        val_subset,\n",
    "        shuffle=True,\n",
    "        batch_size=20,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    \n",
    "    thist, vhist, tmhist, vmhist = fitter.fit(train_subset_dl, val_subset_dl, stone)\n",
    "    \n",
    "    fold_metrics.append({\n",
    "        'fold': i + 1,\n",
    "        'training_history': thist,\n",
    "        'validation_history': vhist,\n",
    "        'training_metrics': tmhist,\n",
    "        'validation_metrics': vmhist\n",
    "    })\n",
    "    \n",
    "    torch.save(fitter.model.state_dict(), f'{vit_kfold_dir}/ViT_{dte}_fold_{i+1}.pt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "[np.array(fold).mean() for fold in fold_metrics[i]['training_history']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics[0]\n",
    "\n",
    "thist_array = []\n",
    "vhist_array = []\n",
    "v_dice_array = []\n",
    "v_precision_array = []\n",
    "v_recall_array = []\n",
    "v_f1score_array = []\n",
    "v_fpr_array = []\n",
    "\n",
    "\n",
    "def get_avg_score(set, metric):\n",
    "    return np.array(fold_metrics[i][set][0][metric]).mean()\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'K-Fold {i+1}')\n",
    "    \n",
    "    thist = [np.array(fold).mean() for fold in fold_metrics[i]['training_history']]\n",
    "    print(f'Training History: {thist}')\n",
    "    thist_array.append(thist)\n",
    "    \n",
    "    vhist = [np.array(fold).mean() for fold in fold_metrics[i]['validation_history']]\n",
    "    print(f'Validation History: {vhist}')\n",
    "    vhist_array.append(vhist)\n",
    "    \n",
    "    # print('\\nTraining Metrics')\n",
    "    # print(f\"Average IOU: {get_avg_score('training_metrics', 'iou_score')}\")\n",
    "    # print(f\"Average Precision: {get_avg_score('training_metrics', 'precision_score')}\")\n",
    "    # print(f\"Average Recall: {get_avg_score('training_metrics', 'recall_score')}\")\n",
    "    # print(f\"Average F1 Score: {get_avg_score('training_metrics', 'f1_score')}\")\n",
    "    \n",
    "    print(f'\\nFold {i+1}: Validation Metrics')\n",
    "    v_dice = get_avg_score('validation_metrics', 'dice_score')\n",
    "    print(f\"Average Dice Score: {v_dice}\")\n",
    "    v_dice_array.append(v_dice)\n",
    "    \n",
    "    v_precision = get_avg_score('validation_metrics', 'precision_score')\n",
    "    print(f\"Average Precision: {v_precision}\")\n",
    "    v_precision_array.append(v_precision)\n",
    "    \n",
    "    v_recall = get_avg_score('validation_metrics', 'recall_score')\n",
    "    print(f\"Average Recall: {v_recall}\")\n",
    "    v_recall_array.append(v_recall)\n",
    "    \n",
    "    v_f1 = get_avg_score('validation_metrics', 'f1_score')\n",
    "    print(f\"Average F1 Score: {v_f1}\")\n",
    "    v_f1score_array.append(v_f1)\n",
    "    \n",
    "    v_f1 = get_avg_score('validation_metrics', 'f1_score')\n",
    "    print(f\"Average F1 Score: {v_f1}\")\n",
    "    v_f1score_array.append(v_f1)\n",
    "    \n",
    "    v_fpr = get_avg_score('validation_metrics', 'fpr')\n",
    "    print(f\"Average False Positive Rate: {v_fpr}\")\n",
    "    v_fpr_array.append(v_fpr)\n",
    "    \n",
    "    print('\\n===================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('K-Fold Cross Validation: Validation Metrics')\n",
    "print(f'Validation - Average Dice Score: {np.array(v_dice_array).mean()} ')\n",
    "print(f'Validation - Average Precision: {np.array(v_precision_array).mean()} ')\n",
    "print(f'Validation - Average Recall: {np.array(v_recall_array).mean()} ')\n",
    "print(f'Validation - Average F1 Score: {np.array(v_f1score_array).mean()} ')\n",
    "print(f'Validation - Average False Positive Rate: {np.array(v_fpr_array).mean()} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5,1, figsize=(5, 15))\n",
    "\n",
    "for i in range(5):\n",
    "    th = np.array(thist_array[i])\n",
    "    vh = np.array(vhist_array[i])\n",
    "    \n",
    "    ax=axes[i]\n",
    "    \n",
    "    sns.lineplot(th, label='Training history', ax=ax)\n",
    "    sns.lineplot(vh, label='Validation history', ax=ax)\n",
    "    \n",
    "    ax.set_title(f'Training and Validation History: Fold {i+1}')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sth = f'history/{dte}_thist.pt'\n",
    "svh = f'history/{dte}_vhist.pt'\n",
    "tk.saved_thist = sth\n",
    "tk.saved_vhist = svh\n",
    "torch.save(thist_array, sth)\n",
    "torch.save(vhist_array, svh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_set,\n",
    "    shuffle=True,\n",
    "    batch_size=20,\n",
    "    collate_fn=collatev2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('K-Fold Cross Validation: Validation Metrics')\n",
    "print(f'Validation - Average Dice Score: {np.array(v_dice_array).mean()} ')\n",
    "print(f'Validation - Average Precision: {np.array(v_precision_array).mean()} ')\n",
    "print(f'Validation - Average Recall: {np.array(v_recall_array).mean()} ')\n",
    "print(f'Validation - Average F1 Score: {np.array(v_f1score_array).mean()} ')\n",
    "print(f'Validation - Average False Positive Rate: {np.array(v_fpr_array).mean()} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dice = []\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1 = []\n",
    "test_fpr = []\n",
    "\n",
    "for filename in os.listdir(vit_kfold_dir):\n",
    "    fitter.model.load_state_dict(torch.load(f'{vit_kfold_dir}/{filename}'))\n",
    "    \n",
    "    h, mh = fitter.validation(test_dataloader)\n",
    "    valmets = pd.DataFrame(mh)\n",
    "    mets = valmets.mean()\n",
    "    \n",
    "    print(f'Version {filename}:')\n",
    "    \n",
    "    test_dice.append(mets.dice_score)\n",
    "    test_precision.append(mets.precision_score)\n",
    "    test_recall.append(mets.recall_score)\n",
    "    test_f1.append(mets.f1_score)\n",
    "    test_fpr.append(mets.fpr)\n",
    "    \n",
    "    print('\\n========================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score = np.array(test_dice).mean()\n",
    "precision_score = np.array(test_precision).mean()\n",
    "recall_score = np.array(test_recall).mean()\n",
    "f1_score = np.array(test_f1).mean()\n",
    "fpr = np.array(test_fpr).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Set Performance Metrics')\n",
    "print(f'Average Dice Score: {dice_score} ')\n",
    "print(f'Average Precision: {precision_score} ')\n",
    "print(f'Average Recall: {recall_score} ')\n",
    "print(f'Average F1 Score: {f1_score} ')\n",
    "print(f'Average FPR: {fpr} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.dice = dice_score\n",
    "tk.precision = precision_score\n",
    "tk.recall = recall_score\n",
    "tk.f1 = f1_score\n",
    "tk.fpr = fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [file for file in os.listdir(vit_kfold_dir)]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.saved_weights = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(enumerate(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = sample[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices, masks, target, path = case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(masks[target].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = slices.squeeze(1).float().to(device)\n",
    "masks = masks.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = stone(slices, masks, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "f.tight_layout()\n",
    "ax1 = a.flat[0]\n",
    "ax2 = a.flat[1]\n",
    "ax1.set_title('MRI Slice Crop')\n",
    "ax2.set_title('Mask Slice Crop')\n",
    "sns.heatmap(x[target].squeeze(), ax=ax1)\n",
    "sns.heatmap(t[target].squeeze(), ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vit(x.flatten(2).to(device), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.view(patch_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "f.tight_layout()\n",
    "ax1 = a.flat[0]\n",
    "ax2 = a.flat[1]\n",
    "ax1.set_title('Mask Prediction')\n",
    "ax2.set_title('Mask Truth')\n",
    "sns.heatmap((y > -0.3).detach().cpu(), ax=ax1, vmax=1)\n",
    "sns.heatmap(t[target].squeeze(), ax=ax2, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.notes = '''\n",
    "- 5 fold cross validation for isa-vit\n",
    "- fixed the reinitialization issue of the fitter\n",
    "- save weights of each fold\n",
    "- evaluate test data using each fold\n",
    "- need to imrpove this by implementing end to end in one notebook (rpn and vit)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('history/runs.csv'):\n",
    "    print('Merging to old df')\n",
    "    prev_df = pd.read_csv('history/runs.csv', index_col='date')\n",
    "    merged = pd.concat([prev_df, tk()])\n",
    "    merged.to_csv('history/runs.csv')\n",
    "else:\n",
    "    print('Making new csv file')\n",
    "    tk().to_csv('history/runs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
