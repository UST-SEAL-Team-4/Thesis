{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fe29d9-78e7-41eb-9d8e-c696e0c39c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.15 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from torch.utils.data import DataLoader\n",
    "from project.preprocessing import NiftiToTensorTransform, z_score_normalization\n",
    "from project.utils import collate_fn, plot_mri_slice, plot_all_slices, plot_all_slices_from_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ed558a-5e3f-4a91-ad4a-59be74b2f4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf78ffeb-207e-4dc2-ab5d-f6ead9981b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03ae679-a626-4b05-a9e5-f6b8a69df871",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ds.load_raw_mri()\n",
    "masks = ds.load_cmb_masks()\n",
    "\n",
    "transform = NiftiToTensorTransform(target_shape = (50, 50), rpn_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28bbd184-e05e-467d-9568-537c206eb781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "dataset = VALDODataset(\n",
    "    cases=cases,\n",
    "    masks=masks,\n",
    "    transform=transform,\n",
    "    normalization=z_score_normalization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed17950-8c3b-4dbb-9c40-d57ff5f452c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bde784f-4fca-43c4-8b04-cc9e5c0cbe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from project.model import RPN\n",
    "\n",
    "config = {\n",
    "    'model': RPN(50**2, 4, 5, 200).to(device),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'device': device,\n",
    "    'epochs': 10,\n",
    "    'loss': nn.SmoothL1Loss(),\n",
    "    # 'loss': nn.MSELoss(),\n",
    "    'lr': 0.0000001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27213c5-1253-4aa6-b6b9-aa561434667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter\n",
    "\n",
    "class RPNFitter(Fitter):\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        # for all samples in train_loader\n",
    "        loss_history = []\n",
    "        for slices, masks, case, counts in train_loader:\n",
    "            num_slices = slices.shape[0]\n",
    "            masks = masks.view(num_slices, 1, -1).float().to(self.device)\n",
    "            # x = slices.view(num_slices, 1, 1, -1).float().to(self.device)\n",
    "            x = slices.view(num_slices, 1, -1).float().to(self.device)\n",
    "            y = []\n",
    "            # feed each slice to rpn\n",
    "            y = self.model(x)\n",
    "            # for slc in x:\n",
    "                # out = self.model(slc)\n",
    "                # y.append(out)\n",
    "                \n",
    "            # y = torch.stack(y)\n",
    "            # calculate loss\n",
    "            losses = self.loss(y, masks)\n",
    "            loss_history.append(losses)\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return loss_history\n",
    "        # requery rpn with \n",
    "        \n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            loss_history = []\n",
    "            # feed all samples\n",
    "            for slices, masks, case, counts in val_loader:\n",
    "                num_slices = slices.shape[0]\n",
    "                masks = masks.float().to(self.device)\n",
    "                x = slices.view(num_slices, 1, 1, -1).float().to(self.device)\n",
    "                y = []\n",
    "                for slc in x:\n",
    "                    out = self.model(slc)\n",
    "                    y.append(out)\n",
    "                y = torch.stack(y)\n",
    "                # calculate loss\n",
    "                losses = self.loss(y, masks)\n",
    "                loss_history.append(losses)\n",
    "            \n",
    "            return loss_history\n",
    "            # get prediction per slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1853759b-3edb-490f-bf3b-3ed966018d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = RPNFitter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac994a6-7edb-4574-9084-da263d341bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = fitter.fit(dloader, dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be57e16-f1cf-4c0a-be74-d28c206c6657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7102,  0.7020,  0.8433,  0.1469,  0.1413,  3.7790,  0.8573,  0.6262,\n",
       "         1.3749,  1.1366,  6.6733,  3.8952,  3.9173,  1.3003,  0.5793,  0.1515,\n",
       "         0.2007,  0.1513,  9.8213,  0.5605,  0.9524,  0.1280,  0.8081,  0.7316,\n",
       "         1.2720,  0.1308,  0.5574,  0.5824,  0.1254,  0.5852,  0.1766,  4.7182,\n",
       "         0.1523,  0.7586,  0.1557,  0.7385,  8.5513,  0.5908,  1.7726,  5.0154,\n",
       "         0.1531,  0.9307,  1.2182,  1.4003,  0.6001,  1.1548,  0.1523,  0.1645,\n",
       "         4.3647,  0.1196,  0.1606,  0.9408,  0.1858,  0.1150,  2.2279,  9.3290,\n",
       "         0.1500,  0.6154,  1.4492,  1.4121,  0.3936,  0.1495,  0.6419,  1.6959,\n",
       "         0.1626,  0.1442, 14.3137,  1.2627, 10.1454,  0.7176,  2.0415,  0.7344],\n",
       "       device='cuda:0', grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(hist[9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
