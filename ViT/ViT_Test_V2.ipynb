{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer for VALDO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import ViTForMaskedImageModeling, ViTFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for VALDO Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALDODataset(Dataset):\n",
    "    def __init__(self, cases, masks, transform):\n",
    "        self.cases = cases\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "        self.cmb_counts = self.count_cmb_per_image(self.masks)\n",
    "\n",
    "        assert len(self.cases) == len(self.masks), 'Cases and masks must have the same length'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            case = self.cases[idx]\n",
    "            mask = self.masks[idx]\n",
    "\n",
    "            slices = []\n",
    "            masks = []\n",
    "\n",
    "        \n",
    "            s, m = self.transform(mri_image_path=case, segmentation_mask_path=mask)\n",
    "            if s is None or m is None:\n",
    "                raise ValueError(f\"Transform returned None for {case} and {mask}\")\n",
    "            \n",
    "            \n",
    "            slices.append(s)\n",
    "            masks.append(m)\n",
    "            \n",
    "            return slices, masks, case, self.cmb_counts[idx]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error loading image: {e}')\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            boxes.append([(x-(w/2.5)), (y-(h/2.5)), ((w+x) + (w/3)), ((h+y) + (h/3))])\n",
    "            # boxes.append([x, y, x +     w, y + h])\n",
    "        return boxes\n",
    "\n",
    "    def count_cmb_per_image(self, segmented_images):\n",
    "        cmb_counts = []\n",
    "        for img_path in segmented_images:\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata()\n",
    "            slice_cmb_counts = [self.extract_bounding_boxes(\n",
    "                (data[:, :, i] > 0).astype(np.uint8)) for i in range(data.shape[2])]\n",
    "            total_cmb_count = sum(len(contours)\n",
    "                                  for contours in slice_cmb_counts)\n",
    "            cmb_counts.append(total_cmb_count)\n",
    "        return cmb_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path):\n",
    "    try:\n",
    "        nifti = nib.load(file_path)\n",
    "        data = nifti.get_fdata()\n",
    "        print(f\"Loaded NIfTI data shape: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NIfTI file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "class NiftiToTensorTransform:\n",
    "    def __init__(self, target_shape=(256, 256), in_channels=1):\n",
    "        self.target_shape = target_shape\n",
    "        self.in_channels = in_channels\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(height=target_shape[0], width=target_shape[1], p=1.0, always_apply=True),\n",
    "            ToTensorV2()\n",
    "        ], is_check_shapes=False)  # Disable shape checking if you are sure about your data consistency\n",
    "\n",
    "    def convert_to_binary_mask(self, segmentation_mask):\n",
    "        binary_mask = (segmentation_mask > 0).astype(np.uint8)\n",
    "        return binary_mask\n",
    "\n",
    "    def __call__(self, mri_image_path, segmentation_mask_path):\n",
    "        try:\n",
    "            # Load the images\n",
    "            mri_image = load_nifti(mri_image_path)\n",
    "            segmentation_mask = load_nifti(segmentation_mask_path)\n",
    "            dim = nib.load(mri_image_path).header['dim'][0]\n",
    "\n",
    "            if mri_image is None or segmentation_mask is None:\n",
    "                raise ValueError(\"Failed to load NIfTI files or data is None.\")\n",
    "\n",
    "            # Convert multi-label mask to binary mask\n",
    "            binary_mask = self.convert_to_binary_mask(segmentation_mask)\n",
    "\n",
    "            if mri_image.shape[0] != dim:\n",
    "                # If the number of channels is not equal to dim, adjust it\n",
    "                mri_image = np.repeat(mri_image, dim, axis=0)\n",
    "            # Apply transformations to the entire volume\n",
    "            augmented = self.transform(image=mri_image, mask=binary_mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "            # # Ensure the number of channels matches the expected input channels\n",
    "            # if image.shape[0] != self.in_channels:\n",
    "            #     raise ValueError(f\"Expected {self.in_channels} input channels, but got {image.shape[0]} channels. Channels should be {self.in_channels}. MRI Image is {mri_image}\")\n",
    "\n",
    "            \n",
    "            # Debugging prints\n",
    "            print(f\"Image shape after transform: {image.shape}\")\n",
    "            print(f\"Mask shape after transform: {mask.shape}\")\n",
    "            print(f\"Unique values in the transformed mask: {torch.unique(mask)}\")\n",
    "\n",
    "            return image, mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error in __call__ with {mri_image_path} and {segmentation_mask_path}: {e}\")\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = NiftiToTensorTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_label_relative = '../VALDO_Dataset/Task2'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "two_directories_up = os.path.abspath(os.path.join(current_directory, \"../\"))\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "testing_label_absolute = os.path.join(\n",
    "    two_directories_up, testing_label_relative)\n",
    "\n",
    "folders = [item for item in os.listdir(testing_label_absolute) if os.path.isdir(\n",
    "    os.path.join(testing_label_absolute, item))]\n",
    "\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in folders:\n",
    "    if \"sub-1\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    elif \"sub-2\" in folder:\n",
    "        cases[\"cohort2\"].append(folder)\n",
    "    else:\n",
    "        cases[\"cohort3\"].append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "cohort2_labels = []\n",
    "cohort2_ids = []\n",
    "for case in cases[\"cohort2\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort2_labels.append(label)\n",
    "    cohort2_ids.append(id)\n",
    "# print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "cohort3_labels = []\n",
    "cohort3_ids = []\n",
    "for case in cases[\"cohort3\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort3_labels.append(label)\n",
    "    cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "all_ids = cohort1_ids + cohort2_ids + cohort3_ids\n",
    "\n",
    "# print(all_labels[0])\n",
    "# print(all_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate for each batch\n",
    "\n",
    "This is used to return the slices, targets, and img_ids during each iteration in the dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    slices = []\n",
    "    targets = []\n",
    "    img_paths = []\n",
    "    cmb_counts = []\n",
    "\n",
    "    for item in batch:\n",
    "        if item is not None:  # Skip None items\n",
    "            item_slices, item_targets, item_img_path, item_cmb_counts = item\n",
    "            slices.extend(item_slices)\n",
    "            targets.extend(item_targets)\n",
    "            img_paths.append(item_img_path)\n",
    "            cmb_counts.append(item_cmb_counts)\n",
    "\n",
    "    if slices:\n",
    "        cases = torch.stack(slices, dim=0)\n",
    "        masks = torch.stack(targets, dim=0)\n",
    "        return cases, masks, img_paths, cmb_counts\n",
    "    else:\n",
    "        return None, None, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = VALDODataset(\n",
    "#     cases=all_ids, masks=all_labels, transform=transform)\n",
    "\n",
    "dataset = VALDODataset(\n",
    "    cases=cohort1_ids, masks=cohort1_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cmb = [1 if count > 0 else 0 for count in dataset.cmb_counts]\n",
    "\n",
    "df_dataset = pd.DataFrame({\n",
    "    'MRI Scans': dataset.cases,\n",
    "    'Segmented Masks': dataset.masks,\n",
    "    'CMB Count': dataset.cmb_counts,\n",
    "    'Has CMB': has_cmb\n",
    "})\n",
    "\n",
    "# df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_dataset, test_size=0.2, stratify=df_dataset['Has CMB'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['MRI Scans'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VALDODataset(train_df['MRI Scans'].tolist(\n",
    "), train_df['Segmented Masks'].tolist(), transform=transform)\n",
    "val_dataset = VALDODataset(val_df['MRI Scans'].tolist(\n",
    "), val_df['Segmented Masks'].tolist(), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 35\n",
    "embed_dim = 35\n",
    "image_size = 256\n",
    "patch_size = (1,32,32)\n",
    "num_layers = 12\n",
    "num_heads = 7\n",
    "mlp_dim = 3072\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "lr = 1e-50\n",
    "num_workers = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, img_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = image_size\n",
    "        self.projection = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size, padding=(0,0,0), dilation=(1,1,1), groups=1, bias=True)  # Corrected padding and dilation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Check and move 'self.projection' weights to the same device as 'x'\n",
    "        if x.device != self.projection.weight.device:\n",
    "            self.projection = self.projection.to(x.device)\n",
    "        \n",
    "        x = self.projection(x)\n",
    "        \n",
    "        print(f\"Shape after projection: {x.shape}\")\n",
    "        # Calculate the number of patches\n",
    "        num_patches = x.shape[0] * x.shape[1]# Assuming x is of shape [batch_size, embed_dim, H, W, D] for 3D\n",
    "        x = x.flatten(2)  # Flatten dimensions H, W, D into one dimension\n",
    "        x = x.transpose(1, 2)  # Swap dimensions to [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        return x, num_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, image_size):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        num_patches_x = image_size // patch_size[1]\n",
    "        num_patches_y = image_size // patch_size[2]\n",
    "        self.num_patches = num_patches_x * num_patches_y\n",
    "        \n",
    "        # Learnable positional embeddings\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(f\"Expected input x to be a tensor, but got {type(x)}\")\n",
    "        \n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "        \n",
    "        # Ensure the dimensions of x match the expected number of patches\n",
    "        if num_patches != self.num_patches:\n",
    "            raise ValueError(f\"Number of patches ({num_patches}) does not match the expected number of patches ({self.num_patches}).\")\n",
    "        \n",
    "        # Move class token and positional embeddings to the same device as x\n",
    "        cls_tokens = self.cls_token.expand(x.size()[0], -1, -1).to(x.device)\n",
    "        position_embedding = self.position_embedding.to(x.device)\n",
    "        \n",
    "        # Concatenate class token with x\n",
    "        print(\"CLS_Tokens\", cls_tokens.shape, \"\\n\", \"Input\", x[0:,:,:].shape)\n",
    "        x = torch.cat((cls_tokens, x[0:,:,:]), dim=1)  # Concatenate along the num_patches dimension, excluding the first cls_token\n",
    "        print(\"Size of x after cat:\", x.size())\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerencoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerencoderLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        \n",
    "        # Move attention and feed_forward layers to the same device as x\n",
    "        self.attention = self.attention.to(device)\n",
    "        self.feed_forward = self.feed_forward.to(device)\n",
    "        self.norm1 = self.norm1.to(device)\n",
    "        self.norm2 = self.norm2.to(device)\n",
    "        self.dropout = self.dropout.to(device)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SegmentationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes, image_size, patch_size):\n",
    "        super(SegmentationHead, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = ((image_size // patch_size[1]) * (image_size // patch_size[2])) # Corrected num_patches calculation\n",
    "        \n",
    "        # Calculate side length of the patches\n",
    "        side_length = int(self.num_patches ** 0.5)\n",
    "        if side_length ** 2 != self.num_patches:\n",
    "            raise ValueError(\"Number of patches is not a perfect square\")\n",
    "        \n",
    "        # Define the Conv3d layer\n",
    "        self.conv = nn.Conv3d(embed_dim, num_classes, kernel_size=(1, side_length, side_length), stride=(1, side_length, side_length), padding=(0, 0, 0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Check and move 'self.conv' weights to the same device as 'x'\n",
    "        if x.device != self.conv.weight.device:\n",
    "            self.conv = self.conv.to(x.device)\n",
    "        \n",
    "        batch_size, num_patches, embed_dim = x.shape\n",
    "        expected_patches = self.num_patches  # No additional token in this context\n",
    "\n",
    "        if num_patches != expected_patches:\n",
    "            raise ValueError(f\"Expected {expected_patches} patches, but got {num_patches}\")\n",
    "\n",
    "        x = x.transpose(1, 2)  # Swap dimensions to [batch_size, embed_dim, num_patches]\n",
    "\n",
    "        side_length = int(self.num_patches ** 0.5)\n",
    "        if side_length ** 2 != self.num_patches:\n",
    "            raise ValueError(\"Number of patches is not a perfect square\")\n",
    "\n",
    "        x = x.view(batch_size, embed_dim, side_length, side_length)  # Reshape to [batch_size, embed_dim, sqrt(num_patches), sqrt(num_patches)]\n",
    "        \n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form the ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerSegmentation(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, num_classes, image_size, num_heads, mlp_dim, num_layers, dropout=0.1):\n",
    "        super(VisionTransformerSegmentation, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(in_channels, patch_size, embed_dim, image_size)\n",
    "        self.position_embedding = PositionEmbedding(embed_dim, patch_size, image_size)\n",
    "        self.transformer_encoder_layers = nn.ModuleList([\n",
    "            TransformerencoderLayer(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.segmentation_head = SegmentationHead(embed_dim, num_classes, image_size, patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        x, num_patches = self.patch_embedding(x)\n",
    "        max_seq_length = num_patches + 1  # Account for the class token\n",
    "        print(f'Patch Embedding output shape: {x.shape}')\n",
    "        x = self.position_embedding(x, max_seq_length)\n",
    "        # print(f'Position Embedding output shape: {x.shape}')\n",
    "        \n",
    "        for layer in self.transformer_encoder_layers:\n",
    "            x = layer(x)\n",
    "            # print(f'Transformer Layer output shape: {x.shape}')\n",
    "        \n",
    "        # print('Segmenting ', x)\n",
    "        x = self.segmentation_head(x)\n",
    "        # print(f'Segmentation Head output shape: {x.shape}')\n",
    "        x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=True)\n",
    "        x = F.interpolate(x, scale_factor=1, mode='bilinear', align_corners=True)\n",
    "\n",
    "        print(f'Final output shape: {x.shape}')\n",
    "        # print(f'Final output: {x}')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,  # drop last one for having same batch size\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in train_loader:\n",
    "    print(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformerSegmentation(in_channels, patch_size, embed_dim, num_classes, image_size, num_heads, mlp_dim, num_layers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        images, masks = batch[0].float().to(device), batch[1].float().to(device)  # Ensure both images and masks are float\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # print(f\"Outputs shape: {outputs.shape}\")  \n",
    "        # print(f\"Masks shape: {masks.shape}\")\n",
    "        \n",
    "        # Reshape masks to match the batch size of outputs\n",
    "        masks = masks.repeat(outputs.size(0), 1, 1, 1)  # Duplicate masks to match batch size\n",
    "        \n",
    "        # Permute outputs to match masks shape\n",
    "        outputs = outputs.permute(0, 2, 3, 1)  \n",
    "        # print(f\"Outputs shape after permute: {outputs.shape}\")     \n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print epoch loss\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    val_loss = 0.0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            val_images, val_masks = val_batch[0].float().to(device), val_batch[1].float().to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            val_outputs = val_outputs.permute(0, 2, 3, 1)  # Permute to match masks shape\n",
    "            val_masks = val_masks.repeat(val_outputs.size(0), 1, 1, 1)  # Duplicate masks to match batch size\n",
    "            val_loss += criterion(val_outputs, val_masks).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    scheduler.step(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
