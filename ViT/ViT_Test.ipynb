{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial tests of ViT for VALDO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch torchvision torchinfo\n",
    "# %pip install accelerate -U\n",
    "# %pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import evaluate\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import Compose, Normalize, Resize \n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import ViTFeatureExtractor, ViTForMaskedImageModeling, ViTImageProcessor\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = 'VALDO_Dataset/Task2'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "parent_directory = os.path.abspath(os.path.join(current_directory, '../../'))\n",
    "\n",
    "# Combine the path\n",
    "testing_label = os.path.join(parent_directory, relative_path)\n",
    "\n",
    "# Dataset folders \n",
    "\n",
    "folders = [case for case in os.listdir(testing_label) if os.path.isdir(os.path.join(testing_label, case))]\n",
    "\n",
    "# Initialize the cohorts \n",
    "\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in folders:\n",
    "    if \"sub-1\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    elif \"sub-2\" in folder:\n",
    "        cases[\"cohort2\"].append(folder)\n",
    "    else:\n",
    "        cases[\"cohort3\"].append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividee the cases according to their cohorts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    label = f\"{testing_label}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "cohort2_labels = []\n",
    "cohort2_ids = []\n",
    "for case in cases[\"cohort2\"]:\n",
    "    label = f\"{testing_label}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort2_labels.append(label)\n",
    "    cohort2_ids.append(id)\n",
    "# print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "cohort3_labels = []\n",
    "cohort3_ids = []\n",
    "for case in cases[\"cohort3\"]:\n",
    "    label = f\"{testing_label}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort3_labels.append(label)\n",
    "    cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "all_ids = cohort1_ids + cohort2_ids + cohort3_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print(all_labels.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print(all_ids.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Dataset class for VALDO \n",
    "\n",
    "**This dataset class is different from the final project since ViT outputs the segmented mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALDODataset(Dataset):\n",
    "    def __init__(self, img_paths, mask_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.cmb_counts = self.count_cmb_per_image(self.mask_paths)\n",
    "        \n",
    "        assert len(self.img_paths) == len(\n",
    "            self.mask_paths), \"Number of images and masks should be same\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.img_paths[idx]\n",
    "            mask_path = self.mask_paths[idx]\n",
    "            cmb_count = self.cmb_counts[idx]\n",
    "\n",
    "            # Load the Image \n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            img = (img / np.max(img) * 255).astype(np.uint8)\n",
    "\n",
    "            # Load the Mask annotations\n",
    "            ann = nib.load(mask_path).get_fdata()\n",
    "            ann = (ann / np.max(ann) * 255).astype(np.uint8)\n",
    "\n",
    "            slices = []\n",
    "            targets = []\n",
    "\n",
    "            for i in range(img.shape[2]):\n",
    "                img_slice = img[:, :, i]\n",
    "                ann_slice = ann[:, :, i]\n",
    "\n",
    "                # Convert single-channel to three-channel \n",
    "                img_slice = cv2.merge([img_slice] *3)\n",
    "\n",
    "                # Augment both image and annotation slice together \n",
    "                img_slice_aug = self.transform(img_slice)\n",
    "                ann_slice_aug = self.transform(ann_slice)\n",
    "                # Convert the mask into tensor\n",
    "                target = torch.tensor(ann_slice_aug, dtype=torch.long)\n",
    "\n",
    "                slices.append(img_slice_aug)\n",
    "                targets.append(target)\n",
    "            \n",
    "            return slices, targets, img_path, cmb_count\n",
    "            # return slices\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            boxes.append([(x-(w/2.5)), (y-(h/2.5)), ((w+x) + (w/3)), ((h+y) + (h/3))])\n",
    "            # boxes.append([x, y, x +     w, y + h])\n",
    "        return boxes\n",
    "\n",
    "    def count_cmb_per_image(self, segmented_images):\n",
    "        cmb_counts = []\n",
    "        for img_path in segmented_images:\n",
    "            img = nib.load(img_path)\n",
    "            data = img.get_fdata()\n",
    "            slice_cmb_counts = [self.extract_bounding_boxes(\n",
    "                (data[:, :, i] > 0).astype(np.uint8)) for i in range(data.shape[2])]\n",
    "            total_cmb_count = sum(len(contours)\n",
    "                                  for contours in slice_cmb_counts)\n",
    "            cmb_counts.append(total_cmb_count)\n",
    "        return cmb_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations used in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = Compose(\n",
    "#     [\n",
    "#         A.Resize(height=512, width = 512, p=1.0),\n",
    "#     ],\n",
    "    \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose(\n",
    "    [\n",
    "        T.ToPILImage(),         # Convert to PIL image  s\n",
    "        T.Resize((512, 512)),   # Resize to the input size required by the ViT model\n",
    "        T.ToTensor(),           # Convert to PyTorch tensor and scale to [0, 1]s\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate for each batch\n",
    "\n",
    "This is used to return the slices, targets, and img_ids during each iteration in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    slices = []\n",
    "    targets = []\n",
    "    img_paths = []\n",
    "    cmb_counts = []\n",
    "\n",
    "    for item in batch:\n",
    "        item_slices, item_targets, item_img_path, item_cmb_counts = item\n",
    "        slices.extend(item_slices)\n",
    "        targets.extend(item_targets)\n",
    "        img_paths.append(item_img_path)\n",
    "        cmb_counts.append(item_cmb_counts)\n",
    "\n",
    "    slices = [torch.stack(tuple(slice_set)) for slice_set in slices]\n",
    "\n",
    "    return slices, targets, img_paths,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading all cohorts to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "dataset = VALDODataset(\n",
    "    img_paths=all_ids, mask_paths=all_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset for the numbers of CMBs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cmb = [1 if count > 0 else 0 for count in dataset.cmb_counts]\n",
    "\n",
    "df_dataset = pd.DataFrame({\n",
    "    'MRI Scans': dataset.img_paths,\n",
    "    'Segmented Masks': dataset.mask_paths,\n",
    "    'CMB Count': dataset.cmb_counts,\n",
    "    'Has CMB': has_cmb\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading ViT feature Extractor and Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import model\n",
    "# model_id = 'google/vit-base-patch16-224-in21k'\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "#     model_id\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation for this model:** https://huggingface.co/learn/computer-vision-course/en/unit3/vision-transformers/vision-transformers-for-image-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature extractor and the model\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForMaskedImageModeling.from_pretrained('google/vit-base-patch16-224-in21k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "_A good sanity check before launching the training is to compute the loss on one sample_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = feature_extractor(images=dataset.__getitem__(1), return_tensors=\"pt\")\n",
    "# example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_dataset, test_size=0.2, stratify=df_dataset['Has CMB'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VALDODataset(train_df['MRI Scans'].tolist(\n",
    "), train_df['Segmented Masks'].tolist(), transform=transform)\n",
    "val_dataset = VALDODataset(val_df['MRI Scans'].tolist(\n",
    "), val_df['Segmented Masks'].tolist(), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Target - ', train_dataset.__getitem__(0)[0])\n",
    "slice, target, img_path, cmb_count = train_dataset.__getitem__(0)\n",
    "print('Slice - ', slice)\n",
    "print('Target - ', target)\n",
    "print('Image Path - ', img_path)\n",
    "print('CMB Count - ', cmb_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dataset = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device will be CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ViTImageProcessor(\n",
    "    ignore_index=0,\n",
    "    do_reduce_labels=False,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Transfer learning involves **freezing** certain layers of the model that have already been trained on a large dataset and fine-tuning other layers on a new (smaller) dataset. To do transfer learning with MaskFormer, the developers proposed the following approach:\n",
    "\n",
    "**Freezing Components**: The Backbone and the Pixel Decoder will be frozen. Their pre-trained weights capture universal features applicable across different datasets and domains.\n",
    "\n",
    "**Training Components**: The _Transformer Decoder_ and _MLP_ will be fine-tuned. This process customizes the segment embeddings and classification layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# If you only want to freeze some layers, you can selectively set requires_grad\n",
    "# For example, freezing the encoder layers:\n",
    "for param in model.vit.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Now, fine-tune only the last layers, such as the head\n",
    "for param in model.vit.layernorm.parameters():\n",
    "    param.requires_grad = True\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layernorm' in name or 'heads' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Check that the parameters are frozen/unfrozen correctly\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {'requires_grad=True' if param.requires_grad else 'requires_grad=False'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluates the given model using the specified dataloader and computes the mean Intersection over Union (IoU).\n",
    "----\n",
    "Args:\n",
    "- model (MaskFormerForInstanceSegmentation): The trained model to be evaluated.\n",
    "- dataloader (DataLoader): DataLoader containing the dataset for evaluation.\n",
    "- preprocessor (AutoImageProcessor): The preprocessor used for post-processing the model outputs.\n",
    "- metric (Any): Metric instance used for calculating IoU.\n",
    "- id2label (dict): Dictionary mapping class ids to their corresponding labels.\n",
    "- max_batches (int, optional): Maximum number of batches to evaluate. If None, evaluates on the entire validation dataset.\n",
    "\n",
    "Returns:\n",
    "float: The mean IoU calculated over the specified number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 1e-5\n",
    "log_interval = 100\n",
    "id2label = {0: 'background', 1: 'CMB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "        model: ViTForMaskedImageModeling, \n",
    "        dataloader: DataLoader,\n",
    "        preprocessor: AutoImageProcessor,\n",
    "        metric: any,\n",
    "        id2label: dict, \n",
    "        max_batches=None,\n",
    "):\n",
    "        model.eval()\n",
    "        running_iou = 0\n",
    "        num_batches = 0 \n",
    "        with torch.no_grad():\n",
    "                for idx, batch in enumerate(tqdm(dataloader)):\n",
    "                        slices, targets = batch\n",
    "                        # Unpack the tuple (assuming batch is a tuple of two tensors)\n",
    "                        if max_batches and idx >= max_batches:\n",
    "                                break\n",
    "                        pixel_values = slices.to(device)\n",
    "                        outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "                        original_images = targets\n",
    "                        target_sizes = [\n",
    "                                (image.shape[0], image.shape[1]) for image in original_images\n",
    "                        ]\n",
    "\n",
    "                        predicted_segmentation_maps = (\n",
    "                                preprocessor.post_process_panoptic_segmentation(\n",
    "                                        outputs, target_sizes=target_sizes\n",
    "                                )\n",
    "                        )\n",
    "\n",
    "                        ground_truth_segmentation_maps = targets\n",
    "                        metric.add_batch(\n",
    "                                predictions=predicted_segmentation_maps,\n",
    "                                references=ground_truth_segmentation_maps,\n",
    "                        )\n",
    "\n",
    "                running_iou += metric.compute(num_labels=len(id2label), ignore_index=0)[\n",
    "                        \"mean_iou\"\n",
    "                ]\n",
    "                num_batches += 1\n",
    "                mean_iou = running_iou / num_batches\n",
    "                return mean_iou\n",
    "\n",
    "def train_model(\n",
    "        model: ViTForMaskedImageModeling, \n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: DataLoader,\n",
    "        preprocessor: AutoImageProcessor,\n",
    "        metric: any,\n",
    "        num_epochs: int,\n",
    "        learning_rate: float,\n",
    "        log_interval: int = 100,\n",
    "        id2label: dict = None,\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Current epoch: {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                slices, targets = batch\n",
    "                outputs = model(\n",
    "                    pixel_values=slices.to(device),\n",
    "                    labels=targets.to(device),\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                batch_size = slices.size(0)\n",
    "                running_loss += loss.item()\n",
    "                num_samples += batch_size\n",
    "\n",
    "                if idx % log_interval == 0 and idx:\n",
    "                    print(f\"Current loss: {running_loss / num_samples}\")\n",
    "\n",
    "                optimizer.step()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {idx}: {e}\")\n",
    "                # continue  # Skip this batch and continue\n",
    "\n",
    "        val_mean_iou = evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=val_dataloader,\n",
    "            preprocessor=preprocessor,\n",
    "            metric=metric,\n",
    "            id2label=dict,\n",
    "        )\n",
    "        print(f\"Validation mIoU: {val_mean_iou}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, batch in enumerate(train_dataloader):\n",
    "#     slices, targets, img_paths = batch\n",
    "#     print(\"Slices\" , slices)\n",
    "#     print(\"targets\", targets)\n",
    "#     print(\"Path\", img_paths)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model, \n",
    "    train_dataloader,\n",
    "    val_dataset,\n",
    "    preprocessor,\n",
    "    metric,\n",
    "    num_epochs=epochs,\n",
    "    log_interval=log_interval,\n",
    "    learning_rate=learning_rate,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
