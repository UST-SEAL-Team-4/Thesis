{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fined tuned fiinal project model version for Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset can be downloaded through this link:** https://www.cse.cuhk.edu.hk/~qdou/cmb-3dcnn/cmb-3dcnn.html?fbclid=IwAR2js3RtsGBi8_6yR7Op95vJ6_lkSZXeOQG0HS7VQPfaN0uArnpa9wdpa_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ensemble_boxes import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import natsort as ns\n",
    "import re\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
    "from effdet.efficientdet import HeadNet\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import Compose, Normalize, Resize, BboxParams\n",
    "import scipy.io as sio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the names of the cases in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_label_relative = './cmb-3dcnn-data'\n",
    "testing_label_relative_t = './cmb-3dcnn-data/nii'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "two_directories_up = os.path.abspath(os.path.join(current_directory, \"../\"))\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "testing_label_absolute = os.path.join(\n",
    "    two_directories_up, testing_label_relative)\n",
    "\n",
    "testing_label_absolute_t = os.path.join(\n",
    "    two_directories_up, testing_label_relative_t)\n",
    "\n",
    "folders = [item for item in os.listdir(testing_label_absolute) if os.path.isdir(\n",
    "    os.path.join(testing_label_absolute, item))]\n",
    "\n",
    "nii = [item for item in os.listdir(testing_label_absolute_t) ]\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in nii:\n",
    "    if \"nii\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    # elif \"sub-2\" in folder:\n",
    "    #     cases[\"cohort2\"].append(folder)\n",
    "    # else:\n",
    "    #     cases[\"cohort3\"].append(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the cases according to their cohorts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    strip = case.rstrip(\".nii\")\n",
    "    # label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    # id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    label = f\"{testing_label_absolute}\\\\ground_truth\\\\{strip}.mat\"\n",
    "    id = f\"{testing_label_absolute}\\\\nii\\\\{strip}.nii\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "# cohort2_labels = []\n",
    "# cohort2_ids = []\n",
    "# for case in cases[\"cohort2\"]:\n",
    "#     label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "#     id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "#     cohort2_labels.append(label)\n",
    "#     cohort2_ids.append(id)\n",
    "# # print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# # print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "# cohort3_labels = []\n",
    "# cohort3_ids = []\n",
    "# for case in cases[\"cohort3\"]:\n",
    "#     label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "#     id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "#     cohort3_labels.append(label)\n",
    "#     cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "# all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "# all_ids = cohort1_ids + cohort2_ids + cohort3_ids\n",
    "\n",
    "all_labels = cohort1_labels\n",
    "all_ids = cohort1_ids\n",
    "\n",
    "# print(all_labels[0])\n",
    "# print(all_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Dataset class for the 2nd dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMB_Dataset(Dataset):\n",
    "    def __init__(self, img_paths, ann_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.ann_paths = ann_paths\n",
    "        self.transform = transform\n",
    "        self.cmb_counts = self.count_cmb_per_image(self.ann_paths)\n",
    "\n",
    "        assert len(self.img_paths) == len(\n",
    "            self.ann_paths), \"Mismatch between number of images and annotations\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.img_paths[idx]\n",
    "            ann_path = self.ann_paths[idx]\n",
    "            cmb_count = self.cmb_counts[idx]\n",
    "\n",
    "            # Load 3D image\n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            img = (img / np.max(img) * 255).astype(np.uint8)\n",
    "\n",
    "            # Load 3D annotation\n",
    "            ann = sio.loadmat(ann_path)['cen']\n",
    "            # ann = (ann > 0).astype(np.uint8)  # Ensure mask is binary\n",
    "\n",
    "            slices = []\n",
    "            targets = []\n",
    "\n",
    "            for i in range(img.shape[2]):\n",
    "                img_slice = img[:, :, i]\n",
    "                ann_slice = []\n",
    "\n",
    "                for a in ann:\n",
    "                    if a[2] == i:\n",
    "                        ann_slice.append([a[0], a[1], a[2]])\n",
    "\n",
    "                # Convert single-channel to three-channel\n",
    "                img_slice = cv2.merge([img_slice] * 3)\n",
    "                boxes = self.extract_bounding_boxes(ann_slice)\n",
    "\n",
    "                if len(boxes) > 0 and self.transform is not None:\n",
    "                    augmented = self.transform(\n",
    "                        image=img_slice, bboxes=boxes, labels=[1]*len(boxes))\n",
    "                    img_slice = augmented['image']\n",
    "                    boxes = augmented['bboxes']\n",
    "                    labels = augmented['labels']\n",
    "                else:\n",
    "                    augmented = self.transform(\n",
    "                        image=img_slice, bboxes=[], labels=[])\n",
    "                    img_slice = augmented['image']\n",
    "                    boxes = augmented['bboxes']\n",
    "                    labels = augmented['labels']\n",
    "\n",
    "                target = {\n",
    "                    'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "                    'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "                }\n",
    "\n",
    "                slices.append(img_slice)\n",
    "                targets.append(target)\n",
    "\n",
    "            return slices, targets, img_path, cmb_count\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        w = 12\n",
    "        h = 12\n",
    "        for m in mask:\n",
    "            x = m[1]\n",
    "            y = m[0]\n",
    "            if x != 0 and y != 0:\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "        return boxes\n",
    "\n",
    "    def count_cmb_per_image(self, segmented_images):\n",
    "        cmb_counts = []\n",
    "        for image in segmented_images:\n",
    "            ann = sio.loadmat(image)['cen']\n",
    "            ann = (ann > 0).astype(np.uint8)\n",
    "            cmb_counts.append(np.sum(ann))\n",
    "        return cmb_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations used in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        A.Resize(height=256, width=256, p=1.0),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ],\n",
    "    p=1.0,\n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        min_area=0,\n",
    "        min_visibility=0,\n",
    "        label_fields=['labels']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate for each batch\n",
    "\n",
    "This is used to return the slices, targets, and img_ids during each iteration in the dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    slices = []\n",
    "    targets = []\n",
    "    img_paths = []\n",
    "    cmb_counts = []\n",
    "\n",
    "    for item in batch:\n",
    "        item_slices, item_targets, item_img_path, item_cmb_counts = item\n",
    "        slices.extend(item_slices)\n",
    "        targets.extend(item_targets)\n",
    "        img_paths.append(item_img_path)\n",
    "        cmb_counts.append(item_cmb_counts)\n",
    "\n",
    "    slices = [torch.stack(tuple(slice_set)) for slice_set in slices]\n",
    "\n",
    "    return slices, targets, img_paths,\n",
    "\n",
    "\n",
    "def euclid_dist(t1, t2):\n",
    "    t1 = np.array(t1)\n",
    "    t2 = np.array(t2)\n",
    "    return np.sqrt(((t1-t2)**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter for the summary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns a dataframe of all the predicted bounding boxes during the validation steps\n",
    "\n",
    "All the returned bounding boxes have a score greater than the score_threshlod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_marking_validation(dataset, prediction_list, slice_num, id, score_threshold):\n",
    "    predicted_cmbs = {\n",
    "        'image_id': [],\n",
    "        'slice_num': [],\n",
    "        'x': [],\n",
    "        'y': [],\n",
    "        'w': [],\n",
    "        'h': []\n",
    "    }\n",
    "\n",
    "    for box in prediction_list:\n",
    "        if box[4].item() > score_threshold:\n",
    "            predicted_cmbs['image_id'].append(id)\n",
    "            predicted_cmbs['slice_num'].append(slice_num)\n",
    "            predicted_cmbs['x'].append(box[0].item())\n",
    "            predicted_cmbs['y'].append(box[1].item())\n",
    "            predicted_cmbs['w'].append(box[2].item())\n",
    "            predicted_cmbs['h'].append(box[3].item())\n",
    "\n",
    "    # Convert to DataFrame once at the end\n",
    "    predicted_cmbs_df = pd.DataFrame(predicted_cmbs)\n",
    "    return predicted_cmbs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns all the ground truth bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_marking(dataset):\n",
    "    all_cmbs = {\n",
    "        'image_id': [],\n",
    "        'slice_num': [],\n",
    "        'x': [],\n",
    "        'y': [],\n",
    "        'w': [],\n",
    "        'h': []\n",
    "    }\n",
    "    for i in range(len(dataset)):\n",
    "        slices, targets, id, count = dataset[i]\n",
    "        for j in range(len(slices)):\n",
    "            for target in targets[j]['boxes']:\n",
    "                all_cmbs['image_id'].append(id)\n",
    "                all_cmbs['slice_num'].append(j)\n",
    "                all_cmbs['x'].append(target[0].item())\n",
    "                all_cmbs['y'].append(target[1].item())\n",
    "                all_cmbs['w'].append(target[2].item())\n",
    "                all_cmbs['h'].append(target[3].item())\n",
    "\n",
    "    # Convert to DataFrame once at the end\n",
    "    all_cmbs = pd.DataFrame(all_cmbs)\n",
    "    return all_cmbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts all the false positives, true positives, and false negatives\n",
    "\n",
    "A dataframe containing the fp, tp, and fp are also returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_FPTP(all_marking, predicted_marking):\n",
    "    # Initialize\n",
    "    fp = pd.DataFrame(columns=['image_id', 'slice_num', 'x', 'y', 'w', 'h'])\n",
    "    fp_count = 0\n",
    "    tp = pd.DataFrame(columns=['image_id', 'slice_num', 'x', 'y', 'w', 'h'])\n",
    "    tp_count = 0\n",
    "    fn = pd.DataFrame(columns=['image_id', 'slice_num', 'x', 'y', 'w', 'h'])\n",
    "    fn_count = 0\n",
    "\n",
    "    # Merge according to image_id and slice_num\n",
    "    merged_df = pd.merge(predicted_marking, all_marking, on=[\n",
    "                         'image_id', 'slice_num'], suffixes=('_pred', '_true'))\n",
    "\n",
    "    # Get the initial false positives\n",
    "    # Create a key for matching\n",
    "    predicted_marking['key'] = predicted_marking['image_id'] + \\\n",
    "        '_' + predicted_marking['slice_num'].astype(str)\n",
    "    merged_df['key'] = merged_df['image_id'] + \\\n",
    "        '_' + merged_df['slice_num'].astype(str)\n",
    "\n",
    "    # Use isin to identify rows not in merged_df\n",
    "    fp = predicted_marking[~predicted_marking['key'].isin(merged_df['key'])]\n",
    "    fp = fp.drop(columns=['key'])\n",
    "    fp_count += len(fp)\n",
    "\n",
    "    grouped_dict = {}\n",
    "\n",
    "    # Group by image_id and slice_num\n",
    "    grouped = merged_df.groupby(['image_id', 'slice_num'])\n",
    "\n",
    "    # Iterate over the groups and store in the dictionary\n",
    "    for (image_id, slice_num), group in grouped:\n",
    "        key = (image_id, slice_num)\n",
    "        grouped_dict[key] = group\n",
    "\n",
    "    # Get all the counts\n",
    "    for key, df in grouped_dict.items():\n",
    "        x_pred_values = df['x_pred'].values\n",
    "        y_pred_values = df['y_pred'].values\n",
    "        x_true_values = df['x_true'].values\n",
    "        y_true_values = df['y_true'].values\n",
    "\n",
    "        w_pred_values = df['w_pred'].values\n",
    "        h_pred_values = df['h_pred'].values\n",
    "\n",
    "        is_correct = False\n",
    "        for i in range(len(x_pred_values)):\n",
    "            pred_cmb = [x_pred_values[i], y_pred_values[i]]\n",
    "            true_cmb = [x_true_values[i], y_true_values[i]]\n",
    "            dist = euclid_dist(pred_cmb, true_cmb)\n",
    "            if dist > 20:\n",
    "                is_correct = False\n",
    "            else:\n",
    "                is_correct = True\n",
    "                break\n",
    "\n",
    "        new_row = {\n",
    "            'image_id': key[0],\n",
    "            'slice_num': key[1],\n",
    "            'x': x_pred_values[i],\n",
    "            'y': y_pred_values[i],\n",
    "            'w': w_pred_values[i],\n",
    "            'h': h_pred_values[i]\n",
    "        }\n",
    "        temp = pd.DataFrame(new_row, index=[0])\n",
    "\n",
    "        if is_correct:\n",
    "            tp_count += 1\n",
    "            tp = pd.concat([tp, temp], ignore_index=True)\n",
    "        else:\n",
    "            fp_count += 1\n",
    "            fp = pd.concat([fp, temp], ignore_index=True)\n",
    "\n",
    "    all_marking['key'] = all_marking['image_id'] + \\\n",
    "        '_' + all_marking['slice_num'].astype(str)\n",
    "    tp['key'] = tp['image_id'] + '_' + tp['slice_num'].astype(str)\n",
    "    # Use isin to identify rows not in tp\n",
    "    fn = all_marking[all_marking['key'].isin(tp['key'])]\n",
    "    fn = fn.drop(columns=['key'])\n",
    "    fn_count += len(fn)\n",
    "\n",
    "    tp = tp.drop(columns=['key'])\n",
    "\n",
    "    return fp, fp_count, tp, tp_count, fn, fn_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized fitter class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.base_dir = f'./{config.folder}'\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10 ** 5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(\n",
    "            self.optimizer, **config.scheduler_params)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        summary_loss_over_itr_train = []\n",
    "        summary_loss_over_itr_val = []\n",
    "        history = []\n",
    "        all_marking = get_all_marking(validation_loader.dataset)\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, step_history = self.train_one_epoch(train_loader)\n",
    "            history.append(step_history)\n",
    "            summary_loss_over_itr_train.append(summary_loss)\n",
    "            self.log(\n",
    "                f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, precision_ave, recall_ave = self.validation(\n",
    "                validation_loader, all_marking)\n",
    "            summary_loss_over_itr_val.append(summary_loss)\n",
    "            # print(len(predictions))\n",
    "            # print(predictions[0].shape)\n",
    "            # print(predictions[0][0].shape)\n",
    "            # # for i in range(predictions)\n",
    "            # for prediction in predictions:\n",
    "            #     for detection in prediction[0]:\n",
    "            #         x_min, y_min, x_max, y_max, confidence, class_label = detection\n",
    "            #         if confidence > 0.7:  # Filter out detections with low confidence\n",
    "            #             print(f\"Detected class {class_label} with confidence {confidence}\")\n",
    "            #             print(f\"Bounding box: ({x_min}, {y_min}) to ({x_max}, {y_max})\")\n",
    "\n",
    "            self.log(\n",
    "                f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}, precision: {precision_ave:.5f}, recall: {recall_ave:.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(\n",
    "                    f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "        return self.best_summary_loss, summary_loss_over_itr_train, summary_loss_over_itr_val, history\n",
    "\n",
    "    def validation(self, val_loader, all_marking):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        for step, (images, targets, image_ids) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' +\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' +\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                batch_size = len(images)\n",
    "                images = torch.stack(images).to(self.device).float()\n",
    "                boxes = [target['boxes'].to(self.device).float()\n",
    "                         for target in targets]\n",
    "                labels = [target['labels'].to(\n",
    "                    self.device).float() for target in targets]\n",
    "\n",
    "                for i in range(len(images)):\n",
    "                    img = images[i].unsqueeze(0)\n",
    "                    bbox = boxes[i]\n",
    "                    cls = labels[i]\n",
    "\n",
    "                    if bbox.nelement() == 0 or cls.nelement() == 0:\n",
    "                        continue\n",
    "\n",
    "                    target = {\n",
    "                        \"bbox\": bbox.unsqueeze(0),\n",
    "                        \"cls\": cls.unsqueeze(0),\n",
    "                        'img_scale': None,\n",
    "                        'img_size': None,\n",
    "                    }\n",
    "\n",
    "                    # loss, _, _, _ = self.model(img, target)\n",
    "                    score_threshold = 0.05\n",
    "                    output = self.model(img, target)\n",
    "                    predicted_marking = get_predicted_marking_validation(\n",
    "                        val_loader.dataset, output['detections'][0], i, image_ids[0], score_threshold=score_threshold)\n",
    "                    if len(predicted_marking) == 0:\n",
    "                        continue\n",
    "                    result = count_FPTP(all_marking, predicted_marking)\n",
    "\n",
    "                    try:\n",
    "                        precision = result[3]/(result[3] + result[1])\n",
    "                    except ZeroDivisionError:\n",
    "                        precision = 0.0\n",
    "\n",
    "                    try:\n",
    "                        recall = result[3]/(result[3] + result[5])\n",
    "                    except ZeroDivisionError:\n",
    "                        recall = 0.0\n",
    "\n",
    "                    precision_list.append(precision)\n",
    "                    recall_list.append(recall)\n",
    "                    summary_loss.update(\n",
    "                        output[\"loss\"].detach().item(), batch_size)\n",
    "\n",
    "        try:\n",
    "            precision_ave = sum(precision_list) / len(precision_list)\n",
    "        except ZeroDivisionError:\n",
    "            precision_ave = 0.0\n",
    "        try:\n",
    "            recall_ave = sum(recall_list) / len(recall_list)\n",
    "        except ZeroDivisionError:\n",
    "            recall_ave = 0.0\n",
    "        return summary_loss, precision_ave, recall_ave\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        history = []\n",
    "\n",
    "        for step, (images, targets, image_ids) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' +\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' +\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            batch_size = len(images[0])\n",
    "            images = [image.to(self.device).float() for image in images]\n",
    "            boxes = [target['boxes'].to(self.device).float()\n",
    "                     for target in targets]\n",
    "            labels = [target['labels'].to(self.device).float()\n",
    "                      for target in targets]\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                img = images[i].unsqueeze(0)\n",
    "                bbox = boxes[i]\n",
    "                cls = labels[i]\n",
    "\n",
    "                # Check if the current slice has any bounding boxes\n",
    "                if bbox.nelement() == 0 or cls.nelement() == 0:\n",
    "                    continue\n",
    "\n",
    "                target = {\n",
    "                    \"bbox\": boxes[i].unsqueeze(0),\n",
    "                    \"cls\": labels[i].unsqueeze(0)\n",
    "                }\n",
    "\n",
    "                output = self.model(img, target)\n",
    "                output['loss'].backward()\n",
    "                summary_loss.update(output['loss'].detach().item(), batch_size)\n",
    "\n",
    "                # TODO: change with appropriate metrics\n",
    "                history.append(summary_loss.avg)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return summary_loss, history\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global config used during the training of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 0\n",
    "    batch_size = 1\n",
    "    # n_epochs = 10\n",
    "    n_epochs = 5\n",
    "    # lr = 0.0001\n",
    "    lr = 0.25e-5\n",
    "\n",
    "    folder = 'Model_Save(Axial)_D7'\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False  # do scheduler.step after optimizer.step\n",
    "    epoch_scheduler = False\n",
    "    # do scheduler.step after validation stage loss -> For scheduler 'ReduceLROnPlateau'\n",
    "    validation_scheduler = True\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "#     scheduler_params = dict(\n",
    "#         max_lr=0.001,\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=2*int(len(train_dataset_aug) / batch_size),\n",
    "#         pct_start=0.31,\n",
    "#         anneal_strategy='cos',\n",
    "#         final_div_factor=10**4\n",
    "#     )\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "#     scheduler_params = dict(\n",
    "#         T_0=5,        # Number of iterations for the first restart.\n",
    "#         T_mult=2,\n",
    "#         eta_min=0.00004,\n",
    "#         last_epoch=-1,\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.ExponentialLR\n",
    "#     scheduler_params = dict(\n",
    "#         gamma = 0.7\n",
    "#     )\n",
    "\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "        verbose=False,\n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "        eps=1e-08\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns the pretrained model\n",
    "\n",
    "- Used the EfficientDet_d7 version\n",
    "- Fine-tuned the number of classes and the image size of the model\n",
    "- The weights used is the same as the one used by the TPE-Det model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "\n",
    "    config = get_efficientdet_config('tf_efficientdet_d7')\n",
    "    config.update({'num_classes': 1})\n",
    "    config.update({'image_size': (256, 256)})\n",
    "    config.update(\n",
    "        {\"url\": \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth\"})\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    # checkpoint = torch.load('efficientdet_d7-f05bf714.pth')\n",
    "    # net.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    # Use default batchnorm\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "\n",
    "    return DetBenchTrain(net, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset for the numbers of CMBs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CMB_Dataset(\n",
    "    img_paths=all_ids, ann_paths=all_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cmb = [1 if count > 0 else 0 for count in dataset.cmb_counts]\n",
    "\n",
    "df_dataset = pd.DataFrame({\n",
    "    'MRI Scans': dataset.img_paths,\n",
    "    'Segmented Masks': dataset.ann_paths,\n",
    "    'CMB Count': dataset.cmb_counts,\n",
    "    'Has CMB': has_cmb\n",
    "})\n",
    "\n",
    "# df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df_dataset, test_size=0.2, stratify=df_dataset['Has CMB'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CMB_Dataset(train_df['MRI Scans'].tolist(\n",
    "), train_df['Segmented Masks'].tolist(), transform=transform)\n",
    "val_dataset = CMB_Dataset(val_df['MRI Scans'].tolist(\n",
    "), val_df['Segmented Masks'].tolist(), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_dataset)):\n",
    "    slices, targets, id, count = val_dataset[i]\n",
    "    for j in range(len(slices)):\n",
    "        for target in targets[j]['boxes']:\n",
    "            if target.nelement() == 0:\n",
    "                print(\"Empty target found!\")\n",
    "            else:\n",
    "                print(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to run the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "\n",
    "    net = get_net()\n",
    "    device = torch.device('cuda')\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=False,  # drop last one for having same batch size\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(val_dataset),\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    print(val_loader.dataset)\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val, history = fitter.fit(\n",
    "        train_loader, val_loader)\n",
    "\n",
    "    return best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val, history = run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that returns the loaded network selected from the trainning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    device = torch.device('cuda')\n",
    "    # Not sure if gagamitin dapat yung d7\n",
    "    config = get_efficientdet_config('tf_efficientdet_d7')\n",
    "\n",
    "    config.update({'num_classes': 1})\n",
    "    config.update({'image_size': (256, 256)})\n",
    "    config.update({\"norm_kwargs\": dict(eps=.001, momentum=.01)})\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    # Configures the classification head of the model\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchPredict(net)\n",
    "    net.eval()\n",
    "    device = torch.device(device)\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the checkpoint to use in the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_axial = load_net('Model_Save(Axial)_D7/best-checkpoint-004epoch.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for making predictions in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence score...? Default 0.22\n",
    "def make_predictions_axial(images, score_threshold=0.07):\n",
    "    device = torch.device('cuda')\n",
    "    images = torch.stack(images).to(device).float()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # det = net_axial(images, torch.tensor([1]*images.shape[0]).float().to(device))\n",
    "        det = net_axial(images)\n",
    "        for i in range(images.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:, :4]\n",
    "            scores = det[i].detach().cpu().numpy()[:, 4]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            # print(indexes)\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "            })\n",
    "    torch.cuda.empty_cache()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CMB_Dataset(\n",
    "    img_paths=all_ids, ann_paths=all_labels, transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    drop_last=False,  # drop last one for having same batch size\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a list of all the predictions in the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "for j, (images_axial, targets_axial, image_ids_axial, _) in enumerate(dataset):\n",
    "    predictions = make_predictions_axial(images_axial)\n",
    "    prediction_list.append({\"predictions\": predictions, \"id\": image_ids_axial})\n",
    "    print(f'Batch {j} prediction done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of predicted boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_boxes = []\n",
    "temp = prediction_list[0]['predictions']\n",
    "for i in range(len(temp)):\n",
    "    predicted_boxes.append(temp[i]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (slice_base, target) in enumerate(zip(slices, targets)):\n",
    "    print(idx, slice_base, target['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the predictions with the ground truth CMB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "case = 9 \n",
    "# Assuming you have defined `targets` and `slices` elsewhere in your code\n",
    "slices, targets, id, count = dataset[case]\n",
    "# Calculate the number of subplots needed based on the length of your data\n",
    "num_slices = len(slices)\n",
    "num_cols = 5\n",
    "# Round up to the nearest integer\n",
    "num_rows = (num_slices + num_cols - 1) // num_cols\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3))\n",
    "# print(targets[8]['boxes'])\n",
    "# Iterate over slices and targets\n",
    "for idx, (slice_base, target) in enumerate(zip(slices, targets)):\n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Generate heatmap\n",
    "    heatmap_data = torch.mean(slice_base.float(), dim=0)\n",
    "    heatmap_data_np = heatmap_data.numpy()\n",
    "    sns.heatmap(heatmap_data_np, ax=ax)\n",
    "\n",
    "    # Generate bounding box\n",
    "    print(idx)\n",
    "    boxes = predicted_boxes[idx]\n",
    "    for box in boxes:\n",
    "        box = torch.from_numpy(box)\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        ax.add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                   linewidth=2, edgecolor='g', facecolor='none'))\n",
    "\n",
    "    boxes = target['boxes']\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        ax.add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                   linewidth=1, edgecolor='b', facecolor='none'))\n",
    "    \n",
    "    ax.set_title(f\"Case {case+1}.nii, Slice {idx}\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the predicted bounding boxes\n",
    "\n",
    "Returns a dataframe of all the prediction with the image_id and the slice_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_marking(dataset, prediction_list):\n",
    "    predicted_cmbs = {\n",
    "        'image_id': [],\n",
    "        'slice_num': [],\n",
    "        'x': [],\n",
    "        'y': [],\n",
    "        'w': [],\n",
    "        'h': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        slices, _, id, count = dataset[i]\n",
    "        predictions = prediction_list[i]['predictions']\n",
    "        for j in range(len(slices)):\n",
    "            for k in range(len(predictions[j]['boxes'])):\n",
    "                box = predictions[j]['boxes'][k]\n",
    "                x_min, y_min, w, h = box\n",
    "                predicted_cmbs['image_id'].append(id)\n",
    "                predicted_cmbs['slice_num'].append(j)\n",
    "                predicted_cmbs['x'].append(x_min)\n",
    "                predicted_cmbs['y'].append(y_min)\n",
    "                predicted_cmbs['w'].append(w)\n",
    "                predicted_cmbs['h'].append(h)\n",
    "\n",
    "    # Convert to DataFrame once at the end\n",
    "    predicted_cmbs_df = pd.DataFrame(predicted_cmbs)\n",
    "    return predicted_cmbs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables for all the ground truth and predicted CMBs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_marking = get_all_marking(dataset)\n",
    "predicted_marking = get_predicted_marking(\n",
    "    dataset, prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes of cases with and without CMB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_cmb = df_dataset[df_dataset['Has CMB'] == 1]\n",
    "df_without_cmb = df_dataset[df_dataset['Has CMB'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_with_cmb = CMB_Dataset(\n",
    "    img_paths=df_with_cmb['MRI Scans'].tolist(),\n",
    "    ann_paths=df_with_cmb['Segmented Masks'].tolist(),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset_without_cmb = CMB_Dataset(\n",
    "    img_paths=df_without_cmb['MRI Scans'].tolist(),\n",
    "    ann_paths=df_without_cmb['Segmented Masks'].tolist(),\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_cmb = count_FPTP(all_marking, predicted_marking)\n",
    "result_without_cmb = count_FPTP(all_marking, predicted_marking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metric(results, dataset_len, cmb_absent=False):\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "\n",
    "    if not cmb_absent:\n",
    "        try:\n",
    "            precision = results[3]/(results[3] + results[1])\n",
    "        except ZeroDivisionError:\n",
    "            precision = 0.0\n",
    "\n",
    "        try:\n",
    "            recall = results[3]/(results[3] + results[5])\n",
    "        except ZeroDivisionError:\n",
    "            recall = 0.0\n",
    "\n",
    "    try:\n",
    "        fp_ave = results[1]/dataset_len\n",
    "    except ZeroDivisionError:\n",
    "        fp_ave = 0.0\n",
    "\n",
    "    return precision, recall, fp_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results for CMBs Present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_cmb_result = performance_metric(\n",
    "    result_with_cmb, len(test_dataset_with_cmb))\n",
    "\n",
    "print('Model\\'s precision:', with_cmb_result[0])\n",
    "print('Model\\'s recall:', with_cmb_result[1])\n",
    "print('Model\\'s fp ave:', with_cmb_result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results for CMBs Absent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_cmb_result = performance_metric(\n",
    "    result_without_cmb, len(test_dataset_without_cmb), True)\n",
    "\n",
    "print('Model\\'s fp ave:', without_cmb_result[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
