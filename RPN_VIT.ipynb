{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from torch.utils.data import DataLoader\n",
    "from project.preprocessing import NiftiToTensorTransform\n",
    "from project.utils import collatev2\n",
    "import winsound\n",
    "from project.utils import compute_statistics\n",
    "from project.evaluation import isa_rpn_metric, isa_vit_metric, Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_tk = Tracker()\n",
    "rpn_tk = Tracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update RPN Weights before running this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_weights = f'fold/Nov_22_2024_195243/fold_2/rpn_fold2.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring and CUDA Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime as dtt\n",
    "import os\n",
    "\n",
    "path = 'logs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "os.makedirs('history', exist_ok=True)\n",
    "rn = dtt.now()\n",
    "dte = rn.strftime('%b_%d_%Y_%H%M%S')\n",
    "\n",
    "logger = logging.getLogger('kess')\n",
    "fh = logging.FileHandler(f'logs/rpn_vit_{dte}.log')\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "\n",
    "dte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_tk.date = rn\n",
    "vit_tk.logfile = f'rpn_vit_{dte}.log'\n",
    "vit_tk.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config for RPN and ViT Fitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.model import RPN, ISAVIT\n",
    "\n",
    "rpn_config = {\n",
    "    'model': RPN(\n",
    "        input_dim=512,\n",
    "        output_dim=4,\n",
    "        image_size=300,\n",
    "        global_context=False,\n",
    "        nh=4,\n",
    "        pretrained=False\n",
    "    ).to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_config = {\n",
    "    'model': ISAVIT(\n",
    "        d_model=512,\n",
    "        patch_size=32,\n",
    "        dim_ff=1600,\n",
    "        global_context=False,\n",
    "    ).to(device),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'device': device,\n",
    "    'epochs': 50,\n",
    "    'loss': nn.BCEWithLogitsLoss(),\n",
    "    'lr': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_tk.model = 'ViT'\n",
    "vit_tk.model_hyperparams = vit_config['model'].config\n",
    "vit_tk.optimizer = f\"{vit_config['optimizer']}\"\n",
    "vit_tk.epochs = vit_config['epochs']\n",
    "vit_tk.loss = f\"{vit_config['loss']}\"\n",
    "vit_tk.lr = vit_config['lr']\n",
    "\n",
    "rpn_tk.model = 'RPN'\n",
    "rpn_tk.model_hyperparams = rpn_config['model'].config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('targets.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('has_microbleed_slice == 1').reset_index(drop=True)\n",
    "vit_tk.only_cmb_slices = True\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Train-Test Split Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_loaders(data,\n",
    "                 cohort,\n",
    "                 test_size=0.2,\n",
    "                 random_state=12,\n",
    "                 target_shape=(300, 300),\n",
    "                 rpn_mode=True,\n",
    "                 logger=None,\n",
    "                 vit_tracker=vit_tk\n",
    "                ):\n",
    "    if cohort == 1:\n",
    "        vit_tracker.cohort1 = True\n",
    "    if cohort == 2:\n",
    "        vit_tracker.cohort2 = True\n",
    "    if cohort == 3:\n",
    "        vit_tracker.cohort3 = True\n",
    "    \n",
    "    vit_tracker.target_shape = target_shape\n",
    "    data = data[data.cohort == cohort]\n",
    "    \n",
    "    s = f'Creating loaders for Cohort {cohort}\\n'\n",
    "\n",
    "    data_train, data_test = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    s += f'TRAIN & TEST: {data_train.shape, data_test.shape}\\n'\n",
    "\n",
    "    paths = data_train.stripped.unique().tolist()\n",
    "    s += f'Total Unique MRI Samples in data_train: {len(paths)}\\n'\n",
    "    \n",
    "    global_min, global_max = compute_statistics(paths)\n",
    "    s += f'GLOBAL MIN & MAX {global_min, global_max}\\n'\n",
    "\n",
    "    transform = NiftiToTensorTransform(\n",
    "        target_shape=target_shape,\n",
    "        rpn_mode=rpn_mode,\n",
    "        normalization=(global_min, global_max)\n",
    "    )\n",
    "\n",
    "    train_set = VALDODataset(\n",
    "        cases=data_train.stripped.tolist(),\n",
    "        masks=data_train.masks.tolist(),\n",
    "        target=data_train.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "    val_set = VALDODataset(\n",
    "        cases=data_test.stripped.tolist(),\n",
    "        masks=data_test.masks.tolist(),\n",
    "        target=data_test.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    if logger != None:\n",
    "        logger.info(s)\n",
    "    else:\n",
    "        print(s)\n",
    "    \n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Fitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter\n",
    "\n",
    "class ViTFitter(Fitter):\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, stage1):\n",
    "        train_history = []\n",
    "        val_history = []\n",
    "        train_metric_history = []\n",
    "        val_metric_history = []\n",
    "        for epoch in range(self.epochs):\n",
    "            self.log(f'EPOCH {epoch} ==============================')\n",
    "            train_loss, train_metric = self.train_one_epoch(train_loader, stage1)\n",
    "            val_loss, val_metric = self.validation(val_loader, stage1)\n",
    "            train_history.append(train_loss)\n",
    "            val_history.append(val_loss)\n",
    "            train_metric_history.append(train_metric)\n",
    "            val_metric_history.append(val_metric)\n",
    "        return train_history, val_history, train_metric_history, val_metric_history\n",
    "\n",
    "    def train_one_epoch(self, train_loader, stage1):\n",
    "        self.model.train()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        counter = 0\n",
    "        for batch in train_loader:\n",
    "            Y = []\n",
    "            T = []\n",
    "            for slices, masks, target, case in batch:\n",
    "                slices = slices.squeeze(1).float().to(self.device)\n",
    "                # slices = slices.squeeze(1).repeat(1, 3, 1, 1).float().to(self.device)\n",
    "                masks = masks.float().to(self.device)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                \n",
    "                # self.log(f'{x.requires_grad}, {t.requires_grad}')\n",
    "                # self.log(f'{x.shape}, {t.shape}')\n",
    "\n",
    "                x = x.flatten(2).float().to(self.device)\n",
    "                t = t.flatten(2).float().to(self.device)\n",
    "                # self.log(f'XT SHAPES: {x.shape}, {t.shape}')\n",
    "                \n",
    "                y = self.model(x, target)\n",
    "                \n",
    "                # print('Prediction:', (y.sigmoid() >= 0.5).int().unique())\n",
    "                # print('Truth:', (t[target] >= 0).int().unique())\n",
    "\n",
    "                dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().squeeze().detach().cpu().numpy() >= 0.5), (t[target].squeeze().detach().cpu().numpy() > 0))\n",
    "\n",
    "                evaluation_metric['dice_score'].append(dice_score)\n",
    "                evaluation_metric['precision_score'].append(precision_score)\n",
    "                evaluation_metric['recall_score'].append(recall_score)\n",
    "                evaluation_metric['f1_score'].append(f1_score)\n",
    "                evaluation_metric['fpr'].append(fpr)\n",
    "                # self.log(f'EVAL METS: {dice_score, precision_score, recall_score, f1_score, fpr}')\n",
    "\n",
    "                Y.append(y)\n",
    "                T.append(t[target])\n",
    "            \n",
    "            losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "            counter += 1\n",
    "            self.log(f'Batch:\\t{counter}/{len(train_loader)}')\n",
    "            self.log(f'Batch samples:\\t{len(batch)}')\n",
    "            self.log(f'Current error:\\t{losses}\\n')\n",
    "            \n",
    "            loss_history.append(losses.detach().cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nTraining Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        return loss_history, evaluation_metric\n",
    "    \n",
    "    def validation(self, val_loader, stage1):\n",
    "        self.model.eval()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        with torch.inference_mode():\n",
    "            for batch in val_loader:\n",
    "                Y = []\n",
    "                T = []\n",
    "                for slices, masks, target, case in batch:\n",
    "                    slices = slices.squeeze(1).float().to(self.device)\n",
    "                    # slices = slices.squeeze(1).repeat(1, 3, 1, 1).float().to(self.device)\n",
    "                    masks = masks.float().to(self.device)\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                    x = x.flatten(2).float().to(self.device)\n",
    "                    t = t.flatten(2).float().to(self.device)\n",
    "                    y = self.model(x, target)\n",
    "\n",
    "                    dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().squeeze().detach().cpu().numpy() >= 0.5), (t[target].squeeze().detach().cpu().numpy() > 0))\n",
    "                    evaluation_metric['dice_score'].append(dice_score)\n",
    "                    evaluation_metric['precision_score'].append(precision_score)\n",
    "                    evaluation_metric['recall_score'].append(recall_score)\n",
    "                    evaluation_metric['f1_score'].append(f1_score)\n",
    "                    evaluation_metric['fpr'].append(fpr)\n",
    "                    Y.append(y)\n",
    "                    T.append(t[target])\n",
    "                \n",
    "                losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "                loss_history.append(losses.cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nValidations Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        return loss_history, evaluation_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_t1, vit_v1 = make_loaders(\n",
    "    data=data,\n",
    "    cohort=1,\n",
    "    rpn_mode=False\n",
    ")\n",
    "\n",
    "vit_t3, vit_v3 = make_loaders(\n",
    "    data=data,\n",
    "    cohort=3,\n",
    "    rpn_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "vit_train_set = ConcatDataset([vit_t1, vit_t3])\n",
    "vit_test_set = ConcatDataset([vit_v1, vit_v3])\n",
    "\n",
    "print(f'ViT Train Set Size: {vit_train_set}')\n",
    "print(f'ViT Test Set Size: {vit_test_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchs = 20\n",
    "\n",
    "vit_test_dataloader = DataLoader(\n",
    "    vit_test_set,\n",
    "    shuffle=True,\n",
    "    batch_size=batchs,\n",
    "    collate_fn=collatev2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True, random_state=45)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.model import Feeder, GCRPN\n",
    "from project.preprocessing import NiftiToTensorTransform, get_transform\n",
    "\n",
    "image_size = 300\n",
    "patch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_test_dice = []\n",
    "vit_test_precision = []\n",
    "vit_test_recall = []\n",
    "vit_test_f1 = []\n",
    "vit_test_fpr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vit_fold_metrics = []\n",
    "\n",
    "\n",
    "fold_dir = f'./fold/{dte}'\n",
    "os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(vit_train_set)):\n",
    "    print(f'Fold {i + 1}/{5}')\n",
    "    logger.info(f'################################### Fold {i+1}/5 ###################################')\n",
    "    \n",
    "    # RPN\n",
    "    # rpn_config['model'] = RPN(**rpn_tk.model_hyperparams).to(device)\n",
    "    rpn = RPN(**rpn_tk.model_hyperparams).to(device)\n",
    "    rpn.load_state_dict(torch.load(rpn_weights))\n",
    "    vit_tk.stage1_weights = rpn_weights\n",
    "\n",
    "    # FEEDER\n",
    "    \n",
    "    resize = get_transform(\n",
    "        height=patch_size,\n",
    "        width=patch_size,\n",
    "        p=1.0,\n",
    "        rpn_mode=False\n",
    "    )\n",
    "\n",
    "    feeder = Feeder(resize)\n",
    "    stone = GCRPN(\n",
    "        rpn=rpn,\n",
    "        feeder=feeder,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size\n",
    "    ).to(device)\n",
    "    \n",
    "    # ViT\n",
    "    \n",
    "    vit_config['model'] = ISAVIT(**vit_tk.model_hyperparams).to(device)\n",
    "    \n",
    "    vit_fitter = ViTFitter(vit_config, logger=logger)\n",
    "    \n",
    "    train_subset = Subset(vit_train_set, train_index)\n",
    "    val_subset = Subset(vit_train_set, val_index)\n",
    "\n",
    "    vit_tk.batch_size = 20\n",
    "    \n",
    "    train_subset_dl = DataLoader(\n",
    "        train_subset,\n",
    "        shuffle=True,\n",
    "        batch_size=vit_tk.batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    \n",
    "    val_subset_dl = DataLoader(\n",
    "        val_subset,\n",
    "        shuffle=True,\n",
    "        batch_size=vit_tk.batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    \n",
    "    vit_thist, vit_vhist, vit_tmhist, vit_vmhist = vit_fitter.fit(train_subset_dl, val_subset_dl, stone)\n",
    "    \n",
    "    vit_fold_metrics.append({\n",
    "        'fold': i + 1,\n",
    "        'training_history': vit_thist,\n",
    "        'validation_history': vit_vhist,\n",
    "        'training_metrics': vit_tmhist,\n",
    "        'validation_metrics': vit_vmhist\n",
    "    })\n",
    "    \n",
    "    vit_h, vit_mh = vit_fitter.validation(vit_test_dataloader, stone)\n",
    "    vit_valmets = pd.DataFrame(vit_mh)\n",
    "    vit_mets = vit_valmets.mean()\n",
    "    \n",
    "    vit_test_dice.append(vit_mets.dice_score)\n",
    "    vit_test_precision.append(vit_mets.precision_score)\n",
    "    vit_test_recall.append(vit_mets.recall_score)\n",
    "    vit_test_f1.append(vit_mets.f1_score)\n",
    "    vit_test_fpr.append(vit_mets.fpr)\n",
    "\n",
    "    os.makedirs(f'{fold_dir}/fold_{i+1}', exist_ok=True)\n",
    "    torch.save(vit_fitter.model.state_dict(), f'{fold_dir}/fold_{i+1}/vit_fold{i+1}.pt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rpn_fold_metrics[i]`\n",
    "\n",
    "- represents each fold\n",
    "\n",
    "`rpn_fold_metrics[1].keys()`\n",
    "\n",
    "- 'fold', 'training_history', 'validation_history', 'training_metrics', 'validation_metrics'\n",
    "\n",
    "`rpn_fold_metrics[1]['training_metrics'][i]`\n",
    "\n",
    "- represents each epoch\n",
    "\n",
    "`rpn_fold_metrics[1]['training_metrics'][1].keys()`\n",
    "\n",
    "- 'iou_score', 'precision_score', 'recall_score', 'f1_score'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_history(fold_metrics, metric, title=None):\n",
    "    avg_train = []\n",
    "    avg_test = []\n",
    "\n",
    "    for fold in range(len(fold_metrics)):\n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        for epoch in range(len(fold_metrics[fold]['training_metrics'])):\n",
    "            th = np.array(fold_metrics[fold]['training_metrics'][epoch][metric]).mean()\n",
    "            train.append(th)\n",
    "            vh = np.array(fold_metrics[fold]['validation_metrics'][epoch][metric]).mean()\n",
    "            test.append(vh)\n",
    "            \n",
    "        avg_train.append(np.array(train).mean())\n",
    "        avg_test.append(np.array(test).mean())\n",
    "            \n",
    "    sns.lineplot(x=range(1, len(avg_train)+1), y=avg_train, label=f'Training {metric}')\n",
    "    sns.lineplot(x=range(1, len(avg_test)+1), y=avg_test, label=f'Validation {metric}')\n",
    "    \n",
    "    plt.title(f'{title} - Training and Validation {metric}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "        \n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_metrics = ['dice_score', 'precision_score', 'recall_score', 'f1_score', 'fpr']\n",
    "\n",
    "for metric in vit_metrics:\n",
    "    plot_metric_history(vit_fold_metrics, metric, 'ViT ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss History Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(fold_metrics, title=None):\n",
    "    avg_train = []\n",
    "    avg_test = []\n",
    "\n",
    "    for fold in range(len(fold_metrics)):\n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        for epoch in range(len(fold_metrics[fold]['training_history'])):\n",
    "            th = np.array(fold_metrics[fold]['training_history'][epoch]).mean()\n",
    "            train.append(th)\n",
    "            vh = np.array(fold_metrics[fold]['validation_history'][epoch]).mean()\n",
    "            test.append(vh)\n",
    "            \n",
    "        avg_train.append(np.array(train).mean())\n",
    "        avg_test.append(np.array(test).mean())\n",
    "        \n",
    "    sns.lineplot(x=range(1, len(avg_train)+1), y=avg_train, label='Training history')\n",
    "    sns.lineplot(x=range(1, len(avg_test)+1), y=avg_test, label='Validation history')\n",
    "        \n",
    "    plt.title(f'{title} - Training and Validation history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(vit_fold_metrics, 'ViT ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rpn_hist = torch.tensor(np.array(rpn_fold_metrics))\n",
    "vit_hist = torch.tensor(np.array(vit_fold_metrics))\n",
    "\n",
    "s_rpn_hist = f'history/{dte}_rpn_hist.pt'\n",
    "s_vit_hist = f'history/{dte}_vit_hist.pt'\n",
    "rpn_tk.saved_thist = s_rpn_hist\n",
    "vit_tk.saved_vhist = s_vit_hist\n",
    "torch.save(rpn_hist, s_rpn_hist)\n",
    "torch.save(vit_hist, s_vit_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vit_metrics = pd.DataFrame(columns=['fold', 'dice', 'precision', 'recall', 'f1', 'fpr'])\n",
    "df_vit_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vit_metrics['fold'] = [1, 2, 3, 4, 5]\n",
    "df_vit_metrics['dice'] = vit_test_dice\n",
    "df_vit_metrics['precision'] = vit_test_precision\n",
    "df_vit_metrics['recall'] = vit_test_recall\n",
    "df_vit_metrics['f1'] = vit_test_f1\n",
    "df_vit_metrics['fpr'] = vit_test_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_tk.dice = df_vit_metrics.dice\n",
    "vit_tk.precision = df_vit_metrics.precision\n",
    "vit_tk.recall = df_vit_metrics.recall\n",
    "vit_tk.f1 = df_vit_metrics.f1\n",
    "vit_tk.fpr = df_vit_metrics.fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vit_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit_iou_score = np.array(vit_test_iou).mean()\n",
    "vit_dice_score = np.array(vit_test_dice).mean()\n",
    "vit_precision_score = np.array(vit_test_precision).mean()\n",
    "vit_recall_score = np.array(vit_test_recall).mean()\n",
    "vit_f1_score = np.array(vit_test_f1).mean()\n",
    "vit_fpr_score = np.array(vit_test_fpr).mean()\n",
    "\n",
    "print('ViT Test Set Performance Metrics')\n",
    "print(f'Average Dice Score: {vit_dice_score} ')\n",
    "print(f'Average Precision: {vit_precision_score} ')\n",
    "print(f'Average Recall: {vit_recall_score} ')\n",
    "print(f'Average F1 Score: {vit_f1_score} ')\n",
    "print(f'Average FPR: {vit_fpr_score} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vit_metrics.to_csv(f'{fold_dir}/vit_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vit_metrics.to_csv(f'./statistical-treatment/rpn_vit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(vit_test_dataloader))\n",
    "\n",
    "for i in range(len(next(iter(vit_test_dataloader)))):\n",
    "    vit_slices, vit_masks, vit_target, vit_case = sample[i]\n",
    "    \n",
    "    vit_slices = vit_slices.squeeze(1).float().to(device)\n",
    "    vit_masks = vit_masks.float().to(device)\n",
    "    \n",
    "    vit_x, vit_t = stone(vit_slices, vit_masks, vit_target)\n",
    "    \n",
    "    f, a = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    f.tight_layout()\n",
    "    ax1 = a.flat[0]\n",
    "    ax2 = a.flat[1]\n",
    "    ax1.set_title('MRI Slice Crop')\n",
    "    ax2.set_title('Mask Slice Crop')\n",
    "    sns.heatmap(vit_x[vit_target].squeeze(), ax=ax1)\n",
    "    sns.heatmap(vit_t[vit_target].squeeze(), ax=ax2)\n",
    "\n",
    "    vit_y = vit_fitter.model(vit_x.flatten(2).to(device), vit_target)\n",
    "    vit_y = vit_y.view(patch_size, patch_size)\n",
    "    \n",
    "    f, a = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    f.tight_layout()\n",
    "    ax1 = a.flat[0]\n",
    "    ax2 = a.flat[1]\n",
    "    ax1.set_title('Mask Prediction')\n",
    "    ax2.set_title('Mask Truth')\n",
    "    sns.heatmap((vit_y > 0.3).detach().cpu(), ax=ax1, vmax=1)\n",
    "    sns.heatmap(vit_t[vit_target].squeeze(), ax=ax2, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_tk.notes = '''\n",
    "Global Context\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_tk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('history/runs.csv'):\n",
    "    print('Merging to old df')\n",
    "\n",
    "    prev_df = pd.read_csv('history/runs.csv', index_col='date')\n",
    "    merged = pd.concat([prev_df, vit_tk()])\n",
    "    merged.to_csv('history/runs.csv')\n",
    "else:\n",
    "    print('Making new csv file')\n",
    "    vit_tk().to_csv('history/runs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
