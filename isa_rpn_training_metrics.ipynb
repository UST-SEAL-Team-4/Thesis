{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from torch.utils.data import DataLoader\n",
    "from project.preprocessing import NiftiToTensorTransform, z_score_normalization, NEWNiftiToTensorTransform\n",
    "from project.utils import collate_fn, plot_mri_slice, plot_all_slices, plot_all_slices_from_array, collatev2\n",
    "import winsound\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from project.utils import memcheck, compute_statistics\n",
    "from project.evaluation import isa_rpn_metric, Tracker, isa_vit_metric\n",
    "from project import PatchTruther, AnchorFeeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime as dtt\n",
    "import os\n",
    "\n",
    "path = 'logs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "os.makedirs('history', exist_ok=True)\n",
    "rn = dtt.now()\n",
    "dte = rn.strftime('%b_%d_%Y_%H%M%S')\n",
    "\n",
    "logger = logging.getLogger('andy')\n",
    "fh = logging.FileHandler(f'logs/{dte}.log')\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "\n",
    "t.date = rn\n",
    "t.logfile = f'{dte}.log'\n",
    "\n",
    "dte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "t.device = device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config for fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.model import RPN\n",
    "\n",
    "config = {\n",
    "    'model': RPN(\n",
    "        input_dim=512,\n",
    "        output_dim=25,\n",
    "        image_size=300,\n",
    "        global_context=True,\n",
    "        nh=4,\n",
    "        # pretrained=True\n",
    "    ).to(device),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'device': device,\n",
    "    'epochs': 2,\n",
    "    'loss': nn.BCEWithLogitsLoss(),\n",
    "    # 'loss': nn.SmoothL1Loss(),\n",
    "    # 'loss': nn.MSELoss(),\n",
    "    # 'loss': nn.L1Loss(),\n",
    "    'lr': 0.0001\n",
    "}\n",
    "\n",
    "t.model = 'RPN'\n",
    "t.model_hyperparams = config['model'].config\n",
    "t.uses_resnet = config['model'].config['pretrained']\n",
    "t.optimizer = f\"{config['optimizer']}\"\n",
    "t.epochs = config['epochs']\n",
    "t.loss = f\"{config['loss']}\"\n",
    "t.lr = config['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pretrained Embedder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = config['model']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.embedder.load_state_dict(torch.load('weights/Encoder_weights_Final_301024012901.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load RPN Weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = config['model']\n",
    "s = 'RPN_weights_241024213949.pt'\n",
    "model.load_state_dict(torch.load(s))\n",
    "t.loaded_weights = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()\n",
    "\n",
    "data = pd.read_csv('targets.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('has_microbleed_slice == 1').reset_index(drop=True)\n",
    "t.only_cmb_slices = True\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader` Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(data, col):\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    iqr = q3-q1\n",
    "    new = data[(data[col] < (q3 + 1.5*iqr)) & (data[col] > (q1 - 1.5*iqr))]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_loaders(data,\n",
    "                 cohort,\n",
    "                 batch_size,\n",
    "                 test_size=0.2,\n",
    "                 random_state=12,\n",
    "                 target_shape=(300, 300),\n",
    "                 rpn_mode=True,\n",
    "                 logger=None,\n",
    "                 tracker=t\n",
    "                ):\n",
    "    if cohort == 1:\n",
    "        t.cohort1 = True\n",
    "    if cohort == 2:\n",
    "        t.cohort2 = True\n",
    "    if cohort == 3:\n",
    "        t.cohort3 = True\n",
    "    t.batch_size = batch_size\n",
    "    t.test_size = test_size\n",
    "    t.target_shape = target_shape\n",
    "    data = data[data.cohort == cohort]\n",
    "    # data = iqr(data, 'max_value')\n",
    "    \n",
    "    s = f'Creating loaders for Cohort {cohort}\\n'\n",
    "\n",
    "    data_train, data_test = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    s += f'TRAIN & TEST: {data_train.shape, data_test.shape}\\n'\n",
    "\n",
    "    paths = data_train.mri.unique().tolist()\n",
    "    s += f'Total Unique MRI Samples in data_train: {len(paths)}\\n'\n",
    "    \n",
    "    global_min, global_max = compute_statistics(paths)\n",
    "    s += f'GLOBAL MIN & MAX {global_min, global_max}\\n'\n",
    "\n",
    "    transform = NEWNiftiToTensorTransform(\n",
    "        target_shape=target_shape,\n",
    "        rpn_mode=rpn_mode,\n",
    "        normalization=(global_min, global_max),\n",
    "        patch_size=target_shape[0]/(tracker.model_hyperparams['output_dim']**.5)\n",
    "    )\n",
    "\n",
    "    trans = NiftiToTensorTransform(\n",
    "        target_shape=target_shape,\n",
    "        rpn_mode=False,\n",
    "        normalization=(global_min, global_max),\n",
    "    )\n",
    "\n",
    "    reference_set = VALDODataset(\n",
    "        cases=data.mri.tolist(),\n",
    "        masks=data.masks.tolist(),\n",
    "        target=data.target.tolist(),\n",
    "        transform=trans\n",
    "    )\n",
    "\n",
    "    train_set = VALDODataset(\n",
    "        cases=data_train.mri.tolist(),\n",
    "        masks=data_train.masks.tolist(),\n",
    "        target=data_train.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "    val_set = VALDODataset(\n",
    "        cases=data_test.mri.tolist(),\n",
    "        masks=data_test.masks.tolist(),\n",
    "        target=data_test.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "\n",
    "    if logger != None:\n",
    "        logger.info(s)\n",
    "    else:\n",
    "        print(s)\n",
    "    \n",
    "    return reference_set, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter\n",
    "\n",
    "class RPNFitter(Fitter):\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        counter = 0\n",
    "        for batch in train_loader:\n",
    "            # self.log('----------------- BATCH -----------------')\n",
    "            Y = []\n",
    "            T = []\n",
    "            for slices, masks, target, case in batch:\n",
    "                # x = slices.squeeze(1).repeat(1, 3, 1, 1).float().to(self.device)\n",
    "                x = slices.squeeze(1).float().to(self.device)\n",
    "                masks = masks.squeeze(1).float().to(self.device)\n",
    "                y = self.model(x, target)\n",
    "\n",
    "                dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().clone().numpy(force=True) >= np.median(y.sigmoid().clone().numpy(force=True))), masks[target].unsqueeze(0).clone().numpy(force=True))\n",
    "\n",
    "                evaluation_metric['dice_score'].append(dice_score)\n",
    "                evaluation_metric['precision_score'].append(precision_score)\n",
    "                evaluation_metric['recall_score'].append(recall_score)\n",
    "                evaluation_metric['f1_score'].append(f1_score)\n",
    "                evaluation_metric['fpr'].append(fpr)\n",
    "                # self.log(f'EVAL METS: {iou_score, precision_score, recall_score, f1_score}')\n",
    "                Y.append(y)\n",
    "                T.append(masks[target].unsqueeze(0))\n",
    "            \n",
    "            losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "            counter += 1\n",
    "            # if counter % len(batch) == 0:\n",
    "            self.log(f'Batch:\\t{counter}/{len(train_loader)}')\n",
    "            self.log(f'Batch samples:\\t{len(batch)}')\n",
    "            self.log(f'Current error:\\t{losses}\\n')\n",
    "            \n",
    "            \n",
    "            loss_history.append(losses.detach().cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nTraining Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        \n",
    "        return loss_history, evaluation_metric\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        with torch.inference_mode():\n",
    "            for batch in val_loader:\n",
    "                Y = []\n",
    "                T = []\n",
    "                for slices, masks, target, case in batch:\n",
    "                    # x = slices.squeeze(1).repeat(1, 3, 1, 1).float().to(self.device)\n",
    "                    x = slices.squeeze(1).float().to(self.device)\n",
    "                    masks = masks.squeeze(1).float().to(self.device)\n",
    "                    y = self.model(x, target)\n",
    "                    \n",
    "                    dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().clone().numpy(force=True) >= np.median(y.sigmoid().clone().numpy(force=True))), (masks[target].unsqueeze(0).clone().numpy(force=True) > 0))\n",
    "                    evaluation_metric['dice_score'].append(dice_score)\n",
    "                    evaluation_metric['precision_score'].append(precision_score)\n",
    "                    evaluation_metric['recall_score'].append(recall_score)\n",
    "                    evaluation_metric['f1_score'].append(f1_score)\n",
    "                    evaluation_metric['fpr'].append(fpr)\n",
    "                    \n",
    "                    Y.append(y)\n",
    "                    T.append(masks[target].unsqueeze(0))\n",
    "                losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "                loss_history.append(losses.cpu().numpy())\n",
    "        self.log(f'\\nValidations Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        return loss_history, evaluation_metric\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = RPNFitter(config, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refset, tl, vl = make_loaders(\n",
    "    data=data,\n",
    "    cohort=1,\n",
    "    batch_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thist, vhist, tmhist, vmhist = fitter.fit(tl, vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "th = torch.tensor(np.array(thist))\n",
    "vh = torch.tensor(np.array(vhist))\n",
    "# print(th.shape)\n",
    "sns.lineplot(th.mean(1), label='Training history')\n",
    "sns.lineplot(vh.mean(1), label='Validation history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sth = f'history/{dte}_thist.pt'\n",
    "svh = f'history/{dte}_vhist.pt'\n",
    "t.saved_thist = sth\n",
    "t.saved_vhist = svh\n",
    "torch.save(th, sth)\n",
    "torch.save(vh, svh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = f'RPN_test15a_weights_{dte}.pt'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.saved_weights = s\n",
    "torch.save(model.state_dict(), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h, mh = fitter.validation(vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valmets = pd.DataFrame(mh)\n",
    "mets = valmets.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.dice = mets.dice_score\n",
    "t.precision = mets.precision_score\n",
    "t.recall = mets.recall_score\n",
    "t.f1 = mets.f1_score\n",
    "t.fpr = mets.fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fitter.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(enumerate(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices, masks, target, case = sample[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = slices.squeeze(1).repeat(1, 3, 1, 1).float().to(device)\n",
    "x = slices.squeeze(1).float().to(device)\n",
    "T = masks.squeeze(1).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sigmoid() > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.loss(y, T[target].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isa_vit_metric((y.sigmoid() > y.sigmoid().median()).numpy(force=True), masks[target].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = AnchorFeeder(t.model_hyperparams['image_size']/(t.model_hyperparams['output_dim']**.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = y.sigmoid().argmax().tolist()\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mris = af(x[target].unsqueeze(0), ts)\n",
    "\n",
    "anns = refset.locate_case_by_mri(case)[1].float()\n",
    "patches = af(anns[target].unsqueeze(0), ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, a = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "sns.heatmap(mris.numpy(force=True), ax=a.flat[0])\n",
    "\n",
    "sns.heatmap(patches, ax=a.flat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    winsound.Beep(500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.notes = '''\n",
    "no important changes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('history/runs.csv'):\n",
    "    print('Merging to old df')\n",
    "    prev_df = pd.read_csv('history/runs.csv', index_col='date')\n",
    "    merged = pd.concat([prev_df, t()])\n",
    "    merged.to_csv('history/runs.csv')\n",
    "else:\n",
    "    print('Making new csv file')\n",
    "    t().to_csv('history/runs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
