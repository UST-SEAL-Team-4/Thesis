{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from project.dataset import VALDODataset\n",
    "from torch.utils.data import DataLoader\n",
    "from project.preprocessing import NiftiToTensorTransform\n",
    "from project.utils import collatev2\n",
    "from project.utils import compute_statistics\n",
    "from project.evaluation import isa_rpn_metric, Tracker, SmoothL1GiouLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_tk = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.6\n",
    "BETA = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring and CUDA Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime as dtt\n",
    "import os\n",
    "\n",
    "path = 'logs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "os.makedirs('history', exist_ok=True)\n",
    "rn = dtt.now()\n",
    "dte = rn.strftime('%b_%d_%Y_%H%M%S')\n",
    "\n",
    "logger = logging.getLogger('kess')\n",
    "fh = logging.FileHandler(f'logs/rpn_vit_{dte}.log')\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "\n",
    "dte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_tk.date = rn\n",
    "rpn_tk.logfile = f'rpn_vit_{dte}.log'\n",
    "rpn_tk.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config for RPN and ViT Fitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.model import RPN, ISAVIT\n",
    "\n",
    "rpn_config = {\n",
    "    'model': RPN(\n",
    "        input_dim=512,\n",
    "        output_dim=4,\n",
    "        image_size=300,\n",
    "        global_context=True,\n",
    "        nh=4,\n",
    "        pretrained=False\n",
    "    ).to(device),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'device': device,\n",
    "    'epochs': 50,\n",
    "    'loss': SmoothL1GiouLoss.loss,\n",
    "    'lr': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_tk.model = 'RPN'\n",
    "rpn_tk.model_hyperparams = rpn_config['model'].config\n",
    "rpn_tk.uses_resnet = rpn_config['model'].config['pretrained']\n",
    "rpn_tk.optimizer = f\"{rpn_config['optimizer']}\"\n",
    "rpn_tk.epochs = rpn_config['epochs']\n",
    "rpn_tk.lr = rpn_config['lr']\n",
    "rpn_tk.loss = f\"\"\"\n",
    "    {rpn_config['loss']}\n",
    "    alpha: {ALPHA}\n",
    "    beta: {BETA}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('targets.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('has_microbleed_slice == 1').reset_index(drop=True)\n",
    "rpn_tk.only_cmb_slices = True\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Train-Test Split Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_loaders(data,\n",
    "                 cohort,\n",
    "                 test_size=0.2,\n",
    "                 random_state=12,\n",
    "                 target_shape=(300, 300),\n",
    "                 rpn_mode=True,\n",
    "                 logger=None,\n",
    "                 rpn_tracker=rpn_tk,\n",
    "                ):\n",
    "    if cohort == 1:\n",
    "        rpn_tracker.cohort1 = True\n",
    "    if cohort == 2:\n",
    "        rpn_tracker.cohort2 = True\n",
    "    if cohort == 3:\n",
    "        rpn_tracker.cohort3 = True\n",
    "    \n",
    "    rpn_tracker.test_size = test_size\n",
    "    data = data[data.cohort == cohort]\n",
    "    \n",
    "    s = f'Creating loaders for Cohort {cohort}\\n'\n",
    "\n",
    "    data_train, data_test = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    s += f'TRAIN & TEST: {data_train.shape, data_test.shape}\\n'\n",
    "\n",
    "    paths = data_train.stripped.unique().tolist()\n",
    "    s += f'Total Unique MRI Samples in data_train: {len(paths)}\\n'\n",
    "    \n",
    "    global_min, global_max = compute_statistics(paths)\n",
    "    s += f'GLOBAL MIN & MAX {global_min, global_max}\\n'\n",
    "\n",
    "    transform = NiftiToTensorTransform(\n",
    "        target_shape=target_shape,\n",
    "        rpn_mode=rpn_mode,\n",
    "        normalization=(global_min, global_max)\n",
    "    )\n",
    "\n",
    "    train_set = VALDODataset(\n",
    "        cases=data_train.stripped.tolist(),\n",
    "        masks=data_train.masks.tolist(),\n",
    "        target=data_train.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "    val_set = VALDODataset(\n",
    "        cases=data_test.stripped.tolist(),\n",
    "        masks=data_test.masks.tolist(),\n",
    "        target=data_test.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    if logger != None:\n",
    "        logger.info(s)\n",
    "    else:\n",
    "        print(s)\n",
    "    \n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN Fitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPNFitter(Fitter):\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'iou_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': []\n",
    "        }\n",
    "        counter = 0\n",
    "        for batch in train_loader:\n",
    "            # self.log('----------------- BATCH -----------------')\n",
    "            Y = []\n",
    "            T = []\n",
    "            for slices, masks, target, case in batch:\n",
    "                # x = slices.squeeze(1).repeat(1, 3, 1, 1).float().to(self.device)\n",
    "                x = slices.squeeze(1).float().to(self.device)\n",
    "                masks = masks.squeeze(1).float().to(self.device)/300\n",
    "                y = self.model(x, target)\n",
    "\n",
    "                iou_score, precision_score, recall_score, f1_score = isa_rpn_metric(image_size=300, target_bbox=masks[target], predicted_bbox=y)\n",
    "                evaluation_metric['iou_score'].append(iou_score)\n",
    "                evaluation_metric['precision_score'].append(precision_score)\n",
    "                evaluation_metric['recall_score'].append(recall_score)\n",
    "                evaluation_metric['f1_score'].append(f1_score)\n",
    "                # self.log(f'EVAL METS: {iou_score, precision_score, recall_score, f1_score}')\n",
    "                Y.append(y)\n",
    "                T.append(masks[target])\n",
    "            \n",
    "            losses= self.loss(pred= torch.stack(Y), target = torch.stack(T), alpha=ALPHA, beta=BETA)\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "            counter += 1\n",
    "            # if counter % len(batch) == 0:\n",
    "            self.log(f'Batch:\\t{counter}/{len(train_loader)}')\n",
    "            self.log(f'Batch samples:\\t{len(batch)}')\n",
    "            self.log(f'Current error:\\t{losses}\\n')\n",
    "            \n",
    "            \n",
    "            loss_history.append(losses.detach().cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nTraining Evaluation Metric:')\n",
    "        self.log(f\"Avg IOU: {sum(evaluation_metric['iou_score']) / len(evaluation_metric['iou_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        \n",
    "        return loss_history, evaluation_metric\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'iou_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': []\n",
    "        }\n",
    "        with torch.inference_mode():\n",
    "            for batch in val_loader:\n",
    "                Y = []\n",
    "                T = []\n",
    "                for slices, masks, target, case in batch:\n",
    "                    # x = slices.squeeze(1).repeat(1, 3, 1, 1).float().to(self.device)\n",
    "                    x = slices.squeeze(1).float().to(self.device)\n",
    "                    masks = masks.squeeze(1).float().to(self.device)/300\n",
    "                    y = self.model(x, target)\n",
    "                    iou_score, precision_score, recall_score, f1_score = isa_rpn_metric(image_size=300, target_bbox=masks[target], predicted_bbox=y)\n",
    "                    evaluation_metric['iou_score'].append(iou_score)\n",
    "                    evaluation_metric['precision_score'].append(precision_score)\n",
    "                    evaluation_metric['recall_score'].append(recall_score)\n",
    "                    evaluation_metric['f1_score'].append(f1_score)\n",
    "                    Y.append(y)\n",
    "                    T.append(masks[target])\n",
    "                losses= self.loss(pred= torch.stack(Y), target = torch.stack(T), alpha=ALPHA, beta=BETA)\n",
    "                loss_history.append(losses.cpu().numpy())\n",
    "        self.log(f'\\nValidation Evaluation Metric:')\n",
    "        self.log(f\"Avg IOU: {sum(evaluation_metric['iou_score']) / len(evaluation_metric['iou_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        return loss_history, evaluation_metric\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_t1, rpn_v1 = make_loaders(\n",
    "    data=data,\n",
    "    cohort=1,\n",
    "    rpn_mode=True\n",
    ")\n",
    "\n",
    "rpn_t3, rpn_v3 = make_loaders(\n",
    "    data=data,\n",
    "    cohort=3,\n",
    "    rpn_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "rpn_train_set = ConcatDataset([rpn_t1, rpn_t3])\n",
    "rpn_test_set = ConcatDataset([rpn_v1, rpn_v3])\n",
    "\n",
    "print(f'RPN Train Set Size: {rpn_train_set}')\n",
    "print(f'RPN Test Set Size: {rpn_test_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchs = 15\n",
    "\n",
    "rpn_test_dataloader = DataLoader(\n",
    "    rpn_test_set,\n",
    "    shuffle=True,\n",
    "    batch_size=batchs,\n",
    "    collate_fn=collatev2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=45)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.model import Feeder, GCRPN\n",
    "from project.preprocessing import NiftiToTensorTransform, get_transform\n",
    "\n",
    "image_size = 300\n",
    "patch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_test_iou = []\n",
    "rpn_test_precision = []\n",
    "rpn_test_recall = []\n",
    "rpn_test_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rpn_fold_metrics = []\n",
    "\n",
    "fold_dir = f'./fold/{dte}'\n",
    "os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(rpn_train_set)):\n",
    "    print(f'Fold {i + 1}/{5}')\n",
    "    logger.info(f'################################### Fold {i+1}/5 ###################################')\n",
    "    \n",
    "    # RPN\n",
    "    rpn_config['model'] = RPN(**rpn_tk.model_hyperparams).to(device)\n",
    "    \n",
    "    rpn_fitter = RPNFitter(rpn_config, logger=logger)\n",
    "    \n",
    "    rpn_train_subset = Subset(rpn_train_set, train_index)\n",
    "    rpn_val_subset = Subset(rpn_train_set, val_index)\n",
    "\n",
    "    rpn_tk.batch_size = 15\n",
    "    \n",
    "    rpn_train_subset_dl = DataLoader(\n",
    "        rpn_train_subset,\n",
    "        shuffle=True,\n",
    "        batch_size=rpn_tk.batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    \n",
    "    rpn_val_subset_dl = DataLoader(\n",
    "        rpn_val_subset,\n",
    "        shuffle=True,\n",
    "        batch_size=rpn_tk.batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    \n",
    "    rpn_thist, rpn_vhist, rpn_tmhist, rpn_vmhist = rpn_fitter.fit(rpn_train_subset_dl, rpn_val_subset_dl)\n",
    "    \n",
    "    rpn_fold_metrics.append({\n",
    "        'fold': i + 1,\n",
    "        'training_history': rpn_thist,\n",
    "        'validation_history': rpn_vhist,\n",
    "        'training_metrics': rpn_tmhist,\n",
    "        'validation_metrics': rpn_vmhist\n",
    "    })\n",
    "    \n",
    "    rpn_h, rpn_mh = rpn_fitter.validation(rpn_test_dataloader)\n",
    "    rpn_valmets = pd.DataFrame(rpn_mh)\n",
    "    rpn_mets = rpn_valmets.mean()\n",
    "    \n",
    "    rpn_test_iou.append(rpn_mets.iou_score)\n",
    "    rpn_test_precision.append(rpn_mets.precision_score)\n",
    "    rpn_test_recall.append(rpn_mets.recall_score)\n",
    "    rpn_test_f1.append(rpn_mets.f1_score)\n",
    "    \n",
    "    os.makedirs(f'{fold_dir}/fold_{i+1}', exist_ok=True)\n",
    "    torch.save(rpn_fitter.model.state_dict(), f'{fold_dir}/fold_{i+1}/rpn_fold{i+1}.pt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rpn_fold_metrics[i]`\n",
    "\n",
    "- represents each fold\n",
    "\n",
    "`rpn_fold_metrics[1].keys()`\n",
    "\n",
    "- 'fold', 'training_history', 'validation_history', 'training_metrics', 'validation_metrics'\n",
    "\n",
    "`rpn_fold_metrics[1]['training_metrics'][i]`\n",
    "\n",
    "- represents each epoch\n",
    "\n",
    "`rpn_fold_metrics[1]['training_metrics'][1].keys()`\n",
    "\n",
    "- 'iou_score', 'precision_score', 'recall_score', 'f1_score'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_history(fold_metrics, metric, title=None):\n",
    "    avg_train = []\n",
    "    avg_test = []\n",
    "\n",
    "    for fold in range(len(fold_metrics)):\n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        for epoch in range(len(fold_metrics[fold]['training_metrics'])):\n",
    "            th = np.array(fold_metrics[fold]['training_metrics'][epoch][metric]).mean()\n",
    "            train.append(th)\n",
    "            vh = np.array(fold_metrics[fold]['validation_metrics'][epoch][metric]).mean()\n",
    "            test.append(vh)\n",
    "            \n",
    "        avg_train.append(np.array(train))\n",
    "        avg_test.append(np.array(test))\n",
    "    \n",
    "    epochs = len(np.mean(avg_train, axis=0)) + 1\n",
    "            \n",
    "    sns.lineplot(x=range(1, epochs), y=np.mean(avg_train, axis=0), label=f'Training {metric}')\n",
    "    sns.lineplot(x=range(1, epochs), y=np.mean(avg_test, axis=0), label=f'Validation {metric}')\n",
    "    \n",
    "    plt.title(f'{title} - Training and Validation {metric}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "        \n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_metrics = ['iou_score', 'precision_score', 'recall_score', 'f1_score']\n",
    "\n",
    "for metric in rpn_metrics:\n",
    "    plot_metric_history(rpn_fold_metrics, metric, 'RPN ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss History Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(fold_metrics, title=None):\n",
    "    avg_train = []\n",
    "    avg_test = []\n",
    "\n",
    "    for fold in range(len(fold_metrics)):\n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        for epoch in range(len(fold_metrics[fold]['training_history'])):\n",
    "            th = np.array(fold_metrics[fold]['training_history'][epoch]).mean()\n",
    "            train.append(th)\n",
    "            vh = np.array(fold_metrics[fold]['validation_history'][epoch]).mean()\n",
    "            test.append(vh)\n",
    "            \n",
    "        avg_train.append(np.array(train))\n",
    "        avg_test.append(np.array(test))\n",
    "    \n",
    "    epochs = len(np.mean(avg_train, axis=0)) + 1\n",
    "        \n",
    "    sns.lineplot(x=range(1, epochs), y=np.mean(avg_train, axis=0), label='Training history')\n",
    "    sns.lineplot(x=range(1, epochs), y=np.mean(avg_test, axis=0), label='Validation history')\n",
    "        \n",
    "    plt.title(f'{title} - Training and Validation history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(rpn_fold_metrics, 'RPN ')\n",
    "# plot_loss_history(vit_fold_metrics, 'ViT ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rpn_hist = torch.tensor(np.array(rpn_fold_metrics))\n",
    "vit_hist = torch.tensor(np.array(vit_fold_metrics))\n",
    "\n",
    "s_rpn_hist = f'history/{dte}_rpn_hist.pt'\n",
    "s_vit_hist = f'history/{dte}_vit_hist.pt'\n",
    "rpn_tk.saved_thist = s_rpn_hist\n",
    "vit_tk.saved_vhist = s_vit_hist\n",
    "torch.save(rpn_hist, s_rpn_hist)\n",
    "torch.save(vit_hist, s_vit_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RPN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpn_metrics = pd.DataFrame(columns=['fold', 'iou', 'precision', 'recall', 'f1'])\n",
    "df_rpn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpn_metrics['fold'] = [1, 2, 3, 4, 5]\n",
    "df_rpn_metrics['iou'] = rpn_test_iou\n",
    "df_rpn_metrics['precision'] = rpn_test_precision\n",
    "df_rpn_metrics['recall'] = rpn_test_recall\n",
    "df_rpn_metrics['f1'] = rpn_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_tk.iou = df_rpn_metrics.iou.mean()\n",
    "rpn_tk.precision = df_rpn_metrics.precision.mean()\n",
    "rpn_tk.recall = df_rpn_metrics.recall.mean()\n",
    "rpn_tk.f1 = df_rpn_metrics.f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_iou_score = np.array(rpn_test_iou).mean()\n",
    "rpn_precision_score = np.array(rpn_test_precision).mean()\n",
    "rpn_recall_score = np.array(rpn_test_recall).mean()\n",
    "rpn_f1_score = np.array(rpn_test_f1).mean()\n",
    "\n",
    "print('RPN Test Set Performance Metrics')\n",
    "print(f'Average IOU: {rpn_iou_score} ')\n",
    "print(f'Average Precision: {rpn_precision_score} ')\n",
    "print(f'Average Recall: {rpn_recall_score} ')\n",
    "print(f'Average F1 Score: {rpn_f1_score} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpn_metrics.to_csv(f'{fold_dir}/rpn_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rpn_metrics.to_csv(f'./statistical-treatment/isa-rpn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(rpn_test_dataloader))\n",
    "\n",
    "for i in range(len(next(iter(rpn_test_dataloader)))):\n",
    "    rpn_slices, rpn_masks, rpn_target, rpn_case = sample[i]\n",
    "    \n",
    "    rpn_x = rpn_slices.squeeze(1).float().to(device)\n",
    "    rpn_T = rpn_masks.squeeze(1).float().to(device)\n",
    "    rpn_y = rpn_fitter.model(rpn_x, rpn_target)\n",
    "    \n",
    "    rpn_fitter.loss(rpn_y, rpn_T[rpn_target])\n",
    "    rpn_bbox = rpn_masks[rpn_target].squeeze().cpu().long()\n",
    "    rpn_y = (rpn_y*300).squeeze().detach().cpu().long()\n",
    "    \n",
    "    ax = sns.heatmap(rpn_x[rpn_target][0].squeeze().cpu(), cmap='gray')\n",
    "\n",
    "    truth = patches.Rectangle(\n",
    "        (rpn_bbox[0], rpn_bbox[1]),\n",
    "        rpn_bbox[2] - rpn_bbox[0],\n",
    "        rpn_bbox[3] - rpn_bbox[1],\n",
    "        linewidth=1, edgecolor='g', facecolor='none'\n",
    "    )\n",
    "\n",
    "    pred = patches.Rectangle(\n",
    "        (rpn_y[0], rpn_y[1]),\n",
    "        rpn_y[2] - rpn_y[0],\n",
    "        rpn_y[3] - rpn_y[1],\n",
    "        linewidth=1, edgecolor='r', facecolor='none'\n",
    "    )\n",
    "\n",
    "    ax.add_patch(truth)\n",
    "    ax.add_patch(pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_tk.notes = '''\n",
    "Global Context\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_tk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('history/runs.csv'):\n",
    "    print('Merging to old df')\n",
    "    prev_df = pd.read_csv('history/runs.csv', index_col='date')\n",
    "    merged = pd.concat([prev_df, rpn_tk()])\n",
    "    merged.to_csv('history/runs.csv')\n",
    "else:\n",
    "    print('Making new csv file')\n",
    "    rpn_tk().to_csv('history/runs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
