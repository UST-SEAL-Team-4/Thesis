{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from project.preprocessing import z_score_normalization, min_max_normalization, NiftiToTensorTransform, get_transform\n",
    "# from project.preprocessing import z_score_normalization, min_max_normalization\n",
    "# from project.training import split_train_val_datasets\n",
    "from project.utils import collate_fn, plot_all_slices, plot_all_slices_from_array, collatev2, compute_statistics\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from project.model import VisionTransformer, ISAVIT\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from project.model.feeder import Feeder\n",
    "import seaborn as sns\n",
    "from project.evaluation import isa_vit_metric, Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime as dtt\n",
    "import os\n",
    "\n",
    "path = 'logs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "os.makedirs('history', exist_ok=True)\n",
    "rn = dtt.now()\n",
    "dte = rn.strftime('%b_%d_%Y_%H%M%S')\n",
    "\n",
    "logger = logging.getLogger('andy')\n",
    "fh = logging.FileHandler(f'logs/{dte}.log')\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "\n",
    "tk.date = rn\n",
    "tk.logfile = f'{dte}.log'\n",
    "\n",
    "dte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tk.device = device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config for fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 300\n",
    "patch_size = 16\n",
    "\n",
    "config = {\n",
    "    'model': ISAVIT(\n",
    "        d_model=512,\n",
    "        patch_size=patch_size,\n",
    "        dim_ff=1600\n",
    "    ).to(device),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'device': device,\n",
    "    'epochs': 5,\n",
    "    'loss': nn.BCEWithLogitsLoss(),\n",
    "    # 'loss': nn.MSELoss(),\n",
    "    'lr': 0.0001\n",
    "}\n",
    "\n",
    "tk.model = 'ViT'\n",
    "tk.model_hyperparams = config['model'].config\n",
    "tk.optimizer = f\"{config['optimizer']}\"\n",
    "tk.epochs = config['epochs']\n",
    "tk.loss = f\"{config['loss']}\"\n",
    "tk.lr = config['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stage 1 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.model import Feeder, RPN, GCRPN\n",
    "\n",
    "resize = get_transform(\n",
    "    height=patch_size,\n",
    "    width=patch_size,\n",
    "    p=1.0,\n",
    "    rpn_mode=False\n",
    ")\n",
    "\n",
    "feeder = Feeder(resize)\n",
    "rpn = RPN(\n",
    "    input_dim=512,\n",
    "    output_dim=4,\n",
    "    image_size=image_size,\n",
    "    nh=4\n",
    ")\n",
    "\n",
    "tk.uses_resnet = rpn.config['resnet']\n",
    "\n",
    "stone = GCRPN(\n",
    "    rpn=rpn,\n",
    "    feeder=feeder,\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stow = 'RPN_test15_weights_Nov_02_2024_192506.pt'\n",
    "tk.stage1_weights = stow\n",
    "stone.rpn.load_state_dict(torch.load(stow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ViT Weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = config['model']\n",
    "s = 'vit_weights_241024213949.pt'\n",
    "tk.loaded_weights = s\n",
    "model.load_state_dict(torch.load(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()\n",
    "\n",
    "data = pd.read_csv('targets.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('has_microbleed_slice == 1').reset_index(drop=True)\n",
    "tk.only_cmb_slices = True\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader` Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(data, col):\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    iqr = q3-q1\n",
    "    new = data[(data[col] < (q3 + 1.5*iqr)) & (data[col] > (q1 - 1.5*iqr))]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_loaders(data,\n",
    "                 cohort,\n",
    "                 batch_size,\n",
    "                 test_size=0.2,\n",
    "                 random_state=12,\n",
    "                 target_shape=(300, 300),\n",
    "                 rpn_mode=True,\n",
    "                 logger=None,\n",
    "                 tracker=tk\n",
    "                ):\n",
    "    if cohort == 1:\n",
    "        tracker.cohort1 = True\n",
    "    if cohort == 2:\n",
    "        tracker.cohort2 = True\n",
    "    if cohort == 3:\n",
    "        tracker.cohort3 = True\n",
    "    tracker.batch_size = batch_size\n",
    "    tracker.test_size = test_size\n",
    "    tracker.target_shape = target_shape\n",
    "    data = data[data.cohort == cohort]\n",
    "    # data = iqr(data, 'max_value')\n",
    "    \n",
    "    s = f'Creating loaders for Cohort {cohort}\\n'\n",
    "\n",
    "    data_train, data_test = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    s += f'TRAIN & TEST: {data_train.shape, data_test.shape}\\n'\n",
    "\n",
    "    paths = data_train.mri.unique().tolist()\n",
    "    s += f'Total Unique MRI Samples in data_train: {len(paths)}\\n'\n",
    "    \n",
    "    global_min, global_max = compute_statistics(paths)\n",
    "    s += f'GLOBAL MIN & MAX {global_min, global_max}\\n'\n",
    "\n",
    "    transform = NiftiToTensorTransform(\n",
    "        target_shape=target_shape,\n",
    "        rpn_mode=rpn_mode,\n",
    "        normalization=(global_min, global_max)\n",
    "    )\n",
    "\n",
    "    train_set = VALDODataset(\n",
    "        cases=data_train.mri.tolist(),\n",
    "        masks=data_train.masks.tolist(),\n",
    "        target=data_train.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "    val_set = VALDODataset(\n",
    "        cases=data_test.mri.tolist(),\n",
    "        masks=data_test.masks.tolist(),\n",
    "        target=data_test.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "\n",
    "    if logger != None:\n",
    "        logger.info(s)\n",
    "    else:\n",
    "        print(s)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter\n",
    "\n",
    "class ViTFitter(Fitter):\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, stage1):\n",
    "        train_history = []\n",
    "        val_history = []\n",
    "        train_metric_history = []\n",
    "        val_metric_history = []\n",
    "        for epoch in range(self.epochs):\n",
    "            self.log(f'EPOCH {epoch} ==============================')\n",
    "            train_loss, train_metric = self.train_one_epoch(train_loader, stage1)\n",
    "            val_loss, val_metric = self.validation(val_loader, stage1)\n",
    "            train_history.append(train_loss)\n",
    "            val_history.append(val_loss)\n",
    "            train_metric_history.append(train_metric)\n",
    "            val_metric_history.append(val_metric)\n",
    "        return train_history, val_history, train_metric_history, val_metric_history\n",
    "\n",
    "    def train_one_epoch(self, train_loader, stage1):\n",
    "        self.model.train()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        counter = 0\n",
    "        for batch in train_loader:\n",
    "            Y = []\n",
    "            T = []\n",
    "            for slices, masks, target, case in batch:\n",
    "                slices = slices.squeeze(1).float().to(self.device)\n",
    "                masks = masks.float().to(self.device)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                \n",
    "                # self.log(f'{x.requires_grad}, {t.requires_grad}')\n",
    "                # self.log(f'{x.shape}, {t.shape}')\n",
    "\n",
    "                x = x.flatten(2).float().to(self.device)\n",
    "                t = t.flatten(2).float().to(self.device)\n",
    "                # self.log(f'XT SHAPES: {x.shape}, {t.shape}')\n",
    "                \n",
    "                y = self.model(x, target)\n",
    "                \n",
    "                # print('Prediction:', (y.sigmoid() >= 0.5).int().unique())\n",
    "                # print('Truth:', (t[target] >= 0).int().unique())\n",
    "\n",
    "                dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().squeeze().detach().cpu().numpy() >= 0.5), (t[target].squeeze().detach().cpu().numpy() > 0))\n",
    "\n",
    "                evaluation_metric['dice_score'].append(dice_score)\n",
    "                evaluation_metric['precision_score'].append(precision_score)\n",
    "                evaluation_metric['recall_score'].append(recall_score)\n",
    "                evaluation_metric['f1_score'].append(f1_score)\n",
    "                evaluation_metric['fpr'].append(fpr)\n",
    "                # self.log(f'EVAL METS: {dice_score, precision_score, recall_score, f1_score, fpr}')\n",
    "\n",
    "                Y.append(y)\n",
    "                T.append(t[target])\n",
    "            \n",
    "            losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "            counter += 1\n",
    "            self.log(f'Batch:\\t{counter}/{len(train_loader)}')\n",
    "            self.log(f'Batch samples:\\t{len(batch)}')\n",
    "            self.log(f'Current error:\\t{losses}\\n')\n",
    "            \n",
    "            loss_history.append(losses.detach().cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nTraining Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        return loss_history, evaluation_metric\n",
    "    \n",
    "    def validation(self, val_loader, stage1):\n",
    "        self.model.eval()\n",
    "        loss_history = []\n",
    "        evaluation_metric = {\n",
    "            'dice_score': [], \n",
    "            'precision_score': [], \n",
    "            'recall_score': [], \n",
    "            'f1_score': [],\n",
    "            'fpr': []\n",
    "        }\n",
    "        with torch.inference_mode():\n",
    "            for batch in val_loader:\n",
    "                Y = []\n",
    "                T = []\n",
    "                for slices, masks, target, case in batch:\n",
    "                    slices = slices.squeeze(1).float().to(self.device)\n",
    "                    masks = masks.float().to(self.device)\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                    x = x.flatten(2).float().to(self.device)\n",
    "                    t = t.flatten(2).float().to(self.device)\n",
    "                    y = self.model(x, target)\n",
    "\n",
    "                    dice_score, precision_score, recall_score, f1_score, fpr = isa_vit_metric((y.sigmoid().squeeze().detach().cpu().numpy() >= 0.5), (t[target].squeeze().detach().cpu().numpy() > 0))\n",
    "                    evaluation_metric['dice_score'].append(dice_score)\n",
    "                    evaluation_metric['precision_score'].append(precision_score)\n",
    "                    evaluation_metric['recall_score'].append(recall_score)\n",
    "                    evaluation_metric['f1_score'].append(f1_score)\n",
    "                    evaluation_metric['fpr'].append(fpr)\n",
    "                    Y.append(y)\n",
    "                    T.append(t[target])\n",
    "                \n",
    "                losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "                loss_history.append(losses.cpu().numpy())\n",
    "        \n",
    "        self.log(f'\\nValidations Evaluation Metric:')\n",
    "        self.log(f\"Avg Dice: {sum(evaluation_metric['dice_score']) / len(evaluation_metric['dice_score'])}\")\n",
    "        self.log(f\"Avg Precision: {sum(evaluation_metric['precision_score']) / len(evaluation_metric['precision_score'])}\")\n",
    "        self.log(f\"Avg Recall: {sum(evaluation_metric['recall_score']) / len(evaluation_metric['recall_score'])}\")\n",
    "        self.log(f\"Avg F1: {sum(evaluation_metric['f1_score']) / len(evaluation_metric['f1_score'])}\")\n",
    "        self.log(f\"Avg FPR: {sum(evaluation_metric['fpr']) / len(evaluation_metric['fpr'])}\\n\")\n",
    "        return loss_history, evaluation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = ViTFitter(config, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl, vl = make_loaders(\n",
    "    data=data,\n",
    "    cohort=1,\n",
    "    rpn_mode=False,\n",
    "    batch_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thist, vhist, tmhist, vmhist = fitter.fit(tl, vl, stone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)\n",
    "winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "th = torch.tensor(np.array(thist))\n",
    "vh = torch.tensor(np.array(vhist))\n",
    "# print(th.shape)\n",
    "sns.lineplot(th.mean(1), label='Training history')\n",
    "sns.lineplot(vh.mean(1), label='Validation history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sth = f'history/{dte}_thist.pt'\n",
    "svh = f'history/{dte}_vhist.pt'\n",
    "tk.saved_thist = sth\n",
    "tk.saved_vhist = svh\n",
    "torch.save(th, sth)\n",
    "torch.save(vh, svh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save The Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = f'vit_weights_{dte}.pt'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config['model']\n",
    "tk.saved_weights = s\n",
    "torch.save(model.state_dict(), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h, mh = fitter.validation(vl, stone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valmets = pd.DataFrame(mh)\n",
    "mets = valmets.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.dice = mets.dice_score\n",
    "tk.precision = mets.precision_score\n",
    "tk.recall = mets.recall_score\n",
    "tk.f1 = mets.f1_score\n",
    "tk.fpr = mets.fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(enumerate(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = sample[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices, masks, target, path = case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(masks[target].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = slices.squeeze(1).float().to(device)\n",
    "masks = masks.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = stone(slices, masks, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "f.tight_layout()\n",
    "ax1 = a.flat[0]\n",
    "ax2 = a.flat[1]\n",
    "ax1.set_title('MRI Slice Crop')\n",
    "ax2.set_title('Mask Slice Crop')\n",
    "sns.heatmap(x[target].squeeze(), ax=ax1)\n",
    "sns.heatmap(t[target].squeeze(), ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vit(x.flatten(2).to(device), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.view(patch_size, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "f.tight_layout()\n",
    "ax1 = a.flat[0]\n",
    "ax2 = a.flat[1]\n",
    "ax1.set_title('Mask Prediction')\n",
    "ax2.set_title('Mask Truth')\n",
    "sns.heatmap((y > -0.3).detach().cpu(), ax=ax1, vmax=1)\n",
    "sns.heatmap(t[target].squeeze(), ax=ax2, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.notes = '''\n",
    "no important changes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('history/runs.csv'):\n",
    "    print('Merging to old df')\n",
    "    prev_df = pd.read_csv('history/runs.csv', index_col='date')\n",
    "    merged = pd.concat([prev_df, tk()])\n",
    "    merged.to_csv('history/runs.csv')\n",
    "else:\n",
    "    print('Making new csv file')\n",
    "    tk().to_csv('history/runs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
