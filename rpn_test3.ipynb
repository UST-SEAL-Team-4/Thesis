{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5f7b93-0e76-44d1-9f4d-bcf691f4cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from torch.utils.data import DataLoader\n",
    "from project.preprocessing import NiftiToTensorTransform, z_score_normalization\n",
    "from project.utils import collate_fn, plot_mri_slice, plot_all_slices, plot_all_slices_from_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f75c27-fda6-412e-88ee-6cea3030f91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70d38fb-bb28-42d5-97b8-a5cfdf3ae7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4231809d-5a19-4327-9c4b-a9b1b407907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ds.load_raw_mri()\n",
    "masks = ds.load_cmb_masks()\n",
    "\n",
    "transform = NiftiToTensorTransform(target_shape = (50, 50), rpn_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01311a48-5910-4acc-9293-7d0db0d8af87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n",
      "INFO:nibabel.global:pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1\n"
     ]
    }
   ],
   "source": [
    "dataset = VALDODataset(\n",
    "    cases=cases,\n",
    "    masks=masks,\n",
    "    transform=transform,\n",
    "    normalization=z_score_normalization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821df1e7-5b92-47cf-af65-cc333b9c9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c508b5-504a-483f-90e6-498863e51aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter\n",
    "\n",
    "class RPNFitter(Fitter):\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        # for all samples in train_loader\n",
    "        loss_history = []\n",
    "        for slices, masks, case, counts in train_loader:\n",
    "            num_slices = slices.shape[0]\n",
    "            masks = masks.view(num_slices, 1, -1).float().to(self.device)\n",
    "            # x = slices.view(num_slices, 1, 1, -1).float().to(self.device)\n",
    "            x = slices.view(num_slices, 1, -1).float().to(self.device)\n",
    "            y = []\n",
    "            # feed each slice to rpn\n",
    "            y = self.model(x)\n",
    "            # for slc in x:\n",
    "                # out = self.model(slc)\n",
    "                # y.append(out)\n",
    "                \n",
    "            # y = torch.stack(y)\n",
    "            # calculate loss\n",
    "            losses = self.loss(y, masks)\n",
    "            loss_history.append(losses)\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return loss_history\n",
    "        # requery rpn with \n",
    "        \n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            loss_history = []\n",
    "            # feed all samples\n",
    "            for slices, masks, case, counts in val_loader:\n",
    "                num_slices = slices.shape[0]\n",
    "                masks = masks.float().to(self.device)\n",
    "                x = slices.view(num_slices, 1, 1, -1).float().to(self.device)\n",
    "                y = []\n",
    "                for slc in x:\n",
    "                    out = self.model(slc)\n",
    "                    y.append(out)\n",
    "                y = torch.stack(y)\n",
    "                # calculate loss\n",
    "                losses = self.loss(y, masks)\n",
    "                loss_history.append(losses)\n",
    "            \n",
    "            return loss_history\n",
    "            # get prediction per slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c42643e-aee9-43d2-9409-cf4239a9d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from project.model import RPN\n",
    "\n",
    "config = {\n",
    "    'model': RPN(50**2, 4, 5, 200).to(device),\n",
    "    'optimizer': torch.optim.SGD,\n",
    "    'device': device,\n",
    "    'epochs': 1,\n",
    "    'loss': nn.SmoothL1Loss(),\n",
    "    # 'loss': nn.MSELoss(),\n",
    "    'lr': 0.0000001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e5bbae1-ccdd-4df1-9906-aff60906585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = RPNFitter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e0ce08-8f45-48ad-8987-4786e782917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = fitter.fit(dloader, dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c291b2-bd33-4880-b9d7-030b68bc32ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9175,  4.5481,  9.7643,  2.2781,  0.3871,  0.4180,  1.2394,  0.9201,\n",
       "         8.6340,  0.4156,  1.5528,  1.9274,  2.4495,  1.3684,  1.5424,  0.3756,\n",
       "         0.4152,  0.8729,  1.4309,  0.3974,  0.4088,  1.2001,  0.4424,  0.3977,\n",
       "         4.0276,  1.4983,  1.0815,  0.8704,  0.8437,  0.3533,  1.1841,  0.9467,\n",
       "         0.6137,  0.7853,  0.8948,  1.7257,  0.3669,  6.7948,  2.0156, 10.1898,\n",
       "         0.3784,  0.9816,  0.4172,  0.4304,  5.1425,  1.0109,  0.3982,  1.5937,\n",
       "         1.4813,  9.4026,  0.4620,  0.9299,  0.3549,  0.3677,  1.3552,  4.8386,\n",
       "         0.8475,  0.4291,  1.8925,  3.9732,  1.5179,  0.8681,  0.9597,  0.8694,\n",
       "         0.9458,  0.4686,  0.7921, 14.0145,  0.4824,  0.3809,  0.8293,  4.0983],\n",
       "       device='cuda:0', grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(hist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e1f38c-bea8-442c-8dc5-98f06aeeb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fitter.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6838646-f143-4967-b2be-2a8dd15daf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices, masks, case, counts = next(enumerate(dloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46945cfb-5fec-43a4-b26c-86b5d7465209",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_slices = slices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df1c8dec-23c9-48ca-9bb1-f78853b95529",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = slices.view(num_slices, 1, -1).float().to(device)\n",
    "y = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
